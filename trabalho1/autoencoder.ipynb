{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## João Pedro Rodrigues Freitas - 11316552"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Fazer os mini batches (train, test)\n",
    "- Fazer o one-hot\n",
    "- conferir normalização\n",
    "- Tirar a seed\n",
    "- Não passar func ativ na ultima camada\n",
    "- Fazer o espelhamento automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, X: np.ndarray) -> None:\n",
    "        self.data = X\n",
    "\n",
    "    def getData(self) -> np.ndarray:\n",
    "        return self.data\n",
    "\n",
    "    def setTrainRatio(self, ratio: float) -> None:\n",
    "        self.train_ratio = ratio\n",
    "\n",
    "    def getTrainRatio(self) -> float:\n",
    "        return self.train_ratio\n",
    "    \n",
    "    def getAttrSize(self) -> int:\n",
    "        return self.data.shape[1]\n",
    "    \n",
    "    def shuffleData(self) -> None:\n",
    "        self.data = np.random.permutation(self.data)\n",
    "\n",
    "    def getTrainData(self) -> np.ndarray:\n",
    "        return self.data[:int(self.data.shape[0] * self.train_ratio)]\n",
    "    \n",
    "    def getTestData(self) -> np.ndarray:\n",
    "        return self.data[int(self.data.shape[0] * self.train_ratio):]\n",
    "\n",
    "    def normalize(self) -> None:\n",
    "        '''Normaliza cada atributo para o intervalo [0, 1]'''\n",
    "        for i in range(self.getAttrSize()):\n",
    "            self.data[:,i] = (self.data[:,i] - np.min(self.data[:,i])) / (np.max(self.data[:,i]) - np.min(self.data[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, n_neurons: int, actv_func, inputs = None, lastLayer = False) -> None:\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.shape = None\n",
    "        self.inputShape = None\n",
    "\n",
    "        self.actv_func = actv_func\n",
    "\n",
    "        self.out = None\n",
    "        self.actv = None\n",
    "\n",
    "        self.lastLayer = lastLayer\n",
    "\n",
    "        self._setWeights()\n",
    "\n",
    "    def _setWeights(self) -> None:  \n",
    "        if self.W is None and self.inputs is not None:\n",
    "            self.inputShape = self.inputs.shape\n",
    "\n",
    "            self.shape = (self.inputShape[-1], self.n_neurons)\n",
    "            # Inicializa os pesos e bias com distribuição uniforme\n",
    "            # entre -0.5 e 0.5\n",
    "            self.W = np.random.rand(self.shape[0], self.shape[1]) - 0.5\n",
    "            self.b = np.random.rand(self.n_neurons) - 0.5\n",
    "\n",
    "    def process(self, inputs):\n",
    "        '''Forward da camada'''\n",
    "        self.inputs = inputs\n",
    "        self._setWeights()\n",
    "        self.out = np.dot(self.inputs, self.W) + self.b\n",
    "        if not self.lastLayer:\n",
    "            self.actv = self.actv_func(self.out)\n",
    "        else:\n",
    "            self.actv = self.out\n",
    "\n",
    "        return self\n",
    "\n",
    "    def getOutput(self):\n",
    "        return self.out\n",
    "    \n",
    "    def getActivation(self):\n",
    "        return self.actv\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    def __init__(self) -> None:\n",
    "        self.layers = np.array([], dtype=Layer)\n",
    "\n",
    "        self.epochs = None\n",
    "        self.lr = None\n",
    "\n",
    "        self.inputs = None # X\n",
    "        self.targets = None # X\n",
    "        self.outputs = [] # X_hat\n",
    "\n",
    "    def addLayer(self, layer: Layer) -> None:\n",
    "        self.layers = np.append(self.layers, layer)\n",
    "\n",
    "    # TODO: separar entre encoder e decoder\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = []\n",
    "        self.outputs.append(inputs)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.process(inputs).getActivation()\n",
    "            self.outputs.append(inputs)\n",
    "\n",
    "            # inputs = layer.getActivation()\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    # TODO: separar entre encoder e decoder\n",
    "    def backward(self) -> None:\n",
    "        i = self.layers.size\n",
    "\n",
    "        x_hat = self.outputs[-1]\n",
    "        err = 2 * (self.targets - x_hat)\n",
    "\n",
    "        # TODO: dividir por n_samples\n",
    "\n",
    "        for layer, x_hat in zip(self.layers[::-1], self.outputs[::-1]):\n",
    "            delta = err\n",
    "\n",
    "            if (i != self.layers.size):\n",
    "                delta *= layer.actv_func(x_hat, derivative=True)\n",
    "\n",
    "            # delta = err * layer.actv_func(x_hat, derivative=True)\n",
    "\n",
    "            err = np.dot(delta, layer.W.T)\n",
    "\n",
    "            if i > 0:\n",
    "                prev_output = self.outputs[i-1]\n",
    "                # dW = np.dot(prev_output.T, delta) / n\n",
    "                # db = np.sum(delta, axis=0) / n\n",
    "                dW = np.dot(prev_output.T, delta)\n",
    "                db = delta.mean()\n",
    "\n",
    "                layer.W += self.lr * dW\n",
    "                layer.b += self.lr * db\n",
    "\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "    # # TODO: separar entre encoder e decoder\n",
    "    # def backward(self) -> None:\n",
    "    #     x_hat = self.outputs[-1]\n",
    "    #     err = 2 * (self.targets - x_hat)\n",
    "    #     i = self.layers.size \n",
    "\n",
    "    #     # TODO: dividir por n_samples\n",
    "\n",
    "    #     for layer, x_hat in zip(self.layers[::-1], self.outputs[::-1]):\n",
    "    #         delta = err\n",
    "\n",
    "    #         if (i != self.layers.size):\n",
    "    #             delta *= layer.actv_func(x_hat, derivative=True)\n",
    "\n",
    "    #         # delta = err * layer.actv_func(x_hat, derivative=True)\n",
    "\n",
    "    #         err = np.dot(delta, layer.W.T)\n",
    "\n",
    "    #         if i > 0:\n",
    "    #             prev_output = self.outputs[i-1]\n",
    "    #             # dW = np.dot(prev_output.T, delta) / n\n",
    "    #             # db = np.sum(delta, axis=0) / n\n",
    "    #             dW = np.dot(prev_output.T, delta)\n",
    "    #             db = delta.mean()\n",
    "\n",
    "    #             layer.W += self.lr * dW\n",
    "    #             layer.b += self.lr * db\n",
    "\n",
    "\n",
    "    #         i -= 1\n",
    "        \n",
    "    def fit(self, inputs, targets, lr: float = 0.01, epochs: int = 100) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.lr = lr\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(inputs)\n",
    "            # error = abs(np.ravel(self.targets - self.outputs[-1])).mean()\n",
    "            # errors.append(error)\n",
    "            # report_progress(epoch, epochs-1,error)\n",
    "            self.backward()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch: {epoch}')\n",
    "                print(f'Loss: {np.mean(np.square(self.targets - self.outputs[-1]))}')\n",
    "\n",
    "\n",
    "    def predict(self, inputs) -> np.ndarray:\n",
    "        self.forward(inputs)\n",
    "        return self.outputs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        return 1 - np.tanh(x)**2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def createModel(layers, actv_func):\n",
    "    model = Autoencoder()\n",
    "\n",
    "    # for l in layers[1:]:\n",
    "    #     layer = Layer(l, actv_func)\n",
    "    #     model.addLayer(layer)\n",
    "\n",
    "    for i in range(1, len(layers)):\n",
    "        layer = Layer(layers[i], actv_func, lastLayer = (i == len(layers)-1))\n",
    "        model.addLayer(layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    X = np.vstack([np.random.normal(1, 0.5, [100,64]),\n",
    "                   np.random.normal(-2, 1, [100,64]),\n",
    "                   np.random.normal(3, 0.75, [100,64])])\n",
    "    \n",
    "    data = Data(X)\n",
    "\n",
    "    data.normalize()\n",
    "    data.shuffleData()\n",
    "    data.setTrainRatio(0.8)\n",
    "\n",
    "    inputDim = data.getAttrSize()\n",
    "    hiddenLayers = 2\n",
    "\n",
    "    # Número de neurônios em cada camada escondida\n",
    "    # A última camada dessa lista é a dimensão latente\n",
    "    neuronsPerLayer = [32, 16]\n",
    "    # [5, 3, 2, 3, 5]\n",
    "\n",
    "    # TODO: fazer espelhar a rede\n",
    "    teste = [32, 16, 32]\n",
    "\n",
    "    autoencoder = createModel([inputDim] + teste + [inputDim], actv_func=sigmoid)\n",
    "    autoencoder.fit(data.getTrainData(), data.getTrainData(), lr=0.01, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 1.0608244165545557\n",
      "Epoch: 10\n",
      "Loss: 0.4084111700687896\n",
      "Epoch: 20\n",
      "Loss: 0.3134902828251414\n",
      "Epoch: 30\n",
      "Loss: 0.27277834126889794\n",
      "Epoch: 40\n",
      "Loss: 2370.5456821758116\n",
      "Epoch: 50\n",
      "Loss: 0.12298787616753062\n",
      "Epoch: 60\n",
      "Loss: 0.12293941649206362\n",
      "Epoch: 70\n",
      "Loss: 0.12290706442641314\n",
      "Epoch: 80\n",
      "Loss: 0.12288546592948216\n",
      "Epoch: 90\n",
      "Loss: 0.12287104660075311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_121215/2027884050.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
