{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## João Pedro Rodrigues Freitas - 11316552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Fazer os mini batches (train, test)\n",
    "- Fazer o one-hot\n",
    "- Fazer normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, X: np.ndarray) -> None:\n",
    "        self.data = X\n",
    "\n",
    "    def getData(self) -> np.ndarray:\n",
    "        return self.data\n",
    "\n",
    "    def setTrainRatio(self, ratio: float) -> None:\n",
    "        self.train_ratio = ratio\n",
    "\n",
    "    def getTrainRatio(self) -> float:\n",
    "        return self.train_ratio\n",
    "    \n",
    "    def getAttrSize(self) -> int:\n",
    "        return self.data.shape[1]\n",
    "    \n",
    "    def shuffleData(self) -> None:\n",
    "        self.data = np.random.permutation(self.data)\n",
    "\n",
    "    def getTrainData(self) -> np.ndarray:\n",
    "        return self.data[:int(self.data.shape[0] * self.train_ratio)]\n",
    "    \n",
    "    def getTestData(self) -> np.ndarray:\n",
    "        return self.data[int(self.data.shape[0] * self.train_ratio):]\n",
    "\n",
    "    def normalize(self) -> None:\n",
    "        '''Normaliza cada atributo para o intervalo [0, 1]'''\n",
    "        for i in range(self.getAttrSize()):\n",
    "            self.data[:,i] = (self.data[:,i] - np.min(self.data[:,i])) / (np.max(self.data[:,i]) - np.min(self.data[:,i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    \n",
    "    def __init__(self, inputDim: int, hiddenLayers: int, neuronsPerLayer: list[int]) -> None:\n",
    "        if len(neuronsPerLayer) != hiddenLayers:\n",
    "            raise ValueError(\"The size of the list must be equal to the number of hidden layers\")\n",
    "\n",
    "        self.inputDim = inputDim\n",
    "        self.hiddenLayers = hiddenLayers\n",
    "        self.neuronsPerLayer = neuronsPerLayer\n",
    "        self.W = []\n",
    "        self.b = [] \n",
    "        self._initParams()\n",
    "\n",
    "    def _initParams(self) -> None:\n",
    "        # layersDim = [input, hidden1, hidden2 (latentSpace), hidden1, output]\n",
    "        layersDim = [self.inputDim] + self.neuronsPerLayer + self.neuronsPerLayer[::-1][1:] + [self.inputDim]\n",
    "\n",
    "        for i in range(1, len(layersDim)):\n",
    "            # inicializa com distribuição uniforme entre -0.5 e 0.5\n",
    "            self.W.append(np.random.rand(layersDim[i-1], layersDim[i]) - 0.5)\n",
    "            self.b.append(np.random.rand(1, layersDim[i]) - 0.5)\n",
    "    \n",
    "    def _sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _der_sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self):\n",
    "        # Encoder\n",
    "        for i in range(self.hiddenLayers):\n",
    "            self.out.append(np.dot(self.ativ[i], self.W[i]) + self.b[i])\n",
    "            self.ativ.append(self._sigmoid(self.out[i+1]))\n",
    "             \n",
    "        # Decoder\n",
    "        for i in range(self.hiddenLayers, 2 * self.hiddenLayers):\n",
    "            self.out.append(np.dot(self.ativ[i], self.W[i]) + self.b[i])\n",
    "            self.ativ.append(self._sigmoid(self.out[i+1]))\n",
    "\n",
    "        print(f'W_Shapes: {[w.shape for w in self.W]}')\n",
    "        print(f'B_Shapes: {[b.shape for b in self.b]}')\n",
    "        print(f'Out_Shapes: {[o.shape for o in self.out]}')\n",
    "        print(f'Ativ_Shapes: {[a.shape for a in self.ativ]}')\n",
    "\n",
    "    def backward(self, lr: float, n: int):\n",
    "        err = 2 * (self.ativ[-1] - self.ativ[0]) # Erro da camada de saída\n",
    "        # Backpropagation Decoder\n",
    "        delta = err * self._der_sigmoid(self.out[-1]) # err * sigm'(Z)\n",
    "        print(\"DELTA\", delta.shape)\n",
    "\n",
    "        for i in range(2 * self.hiddenLayers - 1, self.hiddenLayers - 1, -1):\n",
    "            print(f'Ativ[{i}]: {self.ativ[i].shape}')\n",
    "            dW = np.dot(delta.T, self.ativ[i]) / self.ativ[i].shape[0] # TODO: ou n?\n",
    "\n",
    "            # TODO: axis 0 ou 1?\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / self.ativ[i].shape[0] # TODO: ou n?\n",
    "\n",
    "            self.W[i] -= lr * dW.T\n",
    "            self.b[i] -= lr * db.T\n",
    "\n",
    "            delta = np.dot(self.W[i], delta) * self._der_sigmoid(self.out[i-1])\n",
    "\n",
    "        # Backpropagation Encoder\n",
    "        # delta = np.dot(self.W[self.hiddenLayers], delta) * self._der_sigmoid(self.out[self.hiddenLayers].T)\n",
    "\n",
    "        for i in range(self.hiddenLayers - 1, -1, -1):\n",
    "            dW = np.dot(delta, self.ativ[i]) / self.ativ[i].shape[0]\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / self.ativ[i].shape[0]\n",
    "\n",
    "            self.W[i] -= lr * dW.T\n",
    "            self.b[i] -= lr * db.T\n",
    "\n",
    "            if i != 0:\n",
    "                delta = np.dot(self.W[i], delta) * self._der_sigmoid(self.out[i-1].T)\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, x, lr: float = 0.01, n_epochs: int = 100):\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            n = x.shape[0] # número de exemplos\n",
    "\n",
    "            self.ativ = [x] # Ativacoes\n",
    "            self.out = [x] # outputs, desconsiderar o primeiro\n",
    "\n",
    "            ## Forward\n",
    "            self.forward()\n",
    "            self.backward(lr, n)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}\")\n",
    "                print(f\"Loss: {np.mean(np.square(x - self.ativ[-1]))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    X = np.vstack([np.random.normal(1, 0.5, [100,10]),\n",
    "                   np.random.normal(-2, 1, [100,10]),\n",
    "                   np.random.normal(3, 0.75, [100,10])])\n",
    "\n",
    "    data = Data(X)\n",
    "    \n",
    "    data.normalize()\n",
    "    data.shuffleData()\n",
    "    data.setTrainRatio(0.8)\n",
    "\n",
    "    inputDim = data.getAttrSize()\n",
    "    hiddenLayers = 2\n",
    "    neuronsPerLayer = [5, 3] # Dimensão latente é a ultima camada oculta\n",
    "    # [10, 5, 3, 5, 10]\n",
    "    \n",
    "    autoencoder = Autoencoder(inputDim, hiddenLayers, neuronsPerLayer)\n",
    "    autoencoder.fit(data.getTrainData(), lr = 0.01, n_epochs = 1)\n",
    "\n",
    "    # TODO: Testar o autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_Shapes: [(10, 5), (5, 3), (3, 5), (5, 10)]\n",
      "B_Shapes: [(1, 5), (1, 3), (1, 5), (1, 10)]\n",
      "Out_Shapes: [(240, 10), (240, 5), (240, 3), (240, 5), (240, 10)]\n",
      "Ativ_Shapes: [(240, 10), (240, 5), (240, 3), (240, 5), (240, 10)]\n",
      "DELTA (240, 10)\n",
      "Ativ[3]: (240, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,10) doesn't match the broadcast shape (10,10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[276], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[275], line 18\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# [10, 5, 3, 5, 10]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m Autoencoder(inputDim, hiddenLayers, neuronsPerLayer)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetTrainData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[274], line 88\u001b[0m, in \u001b[0;36mAutoencoder.fit\u001b[0;34m(self, x, lr, n_epochs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m## Forward\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward()\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[274], line 59\u001b[0m, in \u001b[0;36mAutoencoder.backward\u001b[0;34m(self, lr, n)\u001b[0m\n\u001b[1;32m     56\u001b[0m     db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(delta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mativ[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# TODO: ou n?\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m dW\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m db\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     61\u001b[0m     delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[i], delta) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_der_sigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Backpropagation Encoder\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# delta = np.dot(self.W[self.hiddenLayers], delta) * self._der_sigmoid(self.out[self.hiddenLayers].T)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,10) doesn't match the broadcast shape (10,10)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
