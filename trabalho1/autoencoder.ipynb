{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## João Pedro Rodrigues Freitas - 11316552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Esta classe realiza o pre-processamento dos dados, permitindo:\n",
    "- Permite definir uma porcentagem de dados para treino\n",
    "- Embaralhar as linhas\n",
    "- Normalizar os dados (para cada coluna, de acordo com o max e min de cada coluna)\n",
    "- Obter o conjunto de treino\n",
    "- Obter o conjunto de teste\n",
    "- Obter o número de atributos (numero de colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, X: np.ndarray) -> None:\n",
    "        self.data = X\n",
    "\n",
    "\n",
    "    def getData(self) -> np.ndarray:\n",
    "        return self.data\n",
    "    \n",
    "\n",
    "    def setTrainRatio(self, ratio: float) -> None:\n",
    "        self.train_ratio = ratio\n",
    "\n",
    "\n",
    "    def getTrainRatio(self) -> float:\n",
    "        return self.train_ratio\n",
    "    \n",
    "    \n",
    "    def getAttrSize(self) -> int:\n",
    "        return self.data.shape[1]\n",
    "    \n",
    "    \n",
    "    def shuffleData(self) -> None:\n",
    "        self.data = np.random.permutation(self.data)\n",
    "\n",
    "\n",
    "    def getTrainData(self) -> np.ndarray:\n",
    "        return self.data[:int(self.data.shape[0] * self.train_ratio)]\n",
    "    \n",
    "    \n",
    "    def getTestData(self) -> np.ndarray:\n",
    "        return self.data[int(self.data.shape[0] * self.train_ratio):]\n",
    "    \n",
    "\n",
    "    def normalize(self) -> None:\n",
    "        '''Normaliza cada atributo para o intervalo [0, 1]'''\n",
    "        for i in range(self.getAttrSize()):\n",
    "            self.data[:,i] = (self.data[:,i] - np.min(self.data[:,i])) / (np.max(self.data[:,i]) - np.min(self.data[:,i]))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de ativação\n",
    "\n",
    "Foi utilizada a função de ativação sigmoide.\n",
    "\n",
    "A função dada por 1 / (1 + np.exp(-Z)) é sensível a overflow. Desse modo, ela\n",
    "foi adaptada (mesma função utilizada pelo pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "    positives = x >= 0\n",
    "    negatives = ~positives\n",
    "    \n",
    "    exp_x_neg = np.exp(x[negatives])\n",
    "    \n",
    "    y = x.copy()\n",
    "    y[positives] = 1 / (1 + np.exp(-x[positives]))\n",
    "    y[negatives] = exp_x_neg / (1 + exp_x_neg)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer\n",
    "Os layers contém as informações das camadas.\n",
    "\n",
    "Os pesos e bias são inicializados de acordo com uma distribuição uniforme\n",
    "no intervalo de -0.5 a 0.5\n",
    "\n",
    "O layer permite realizar o forward de si próprio. \n",
    "\n",
    "A função de ativação utilizada como padrão é a sigmoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    # Construtor da classe\n",
    "    def __init__(self, n_neurons: int, inputs = None, lastLayer: bool = False) -> None:\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.out = None # Saída da camada\n",
    "        self.actv = None # Saída com função de ativação\n",
    "\n",
    "        self.inputs = inputs # Entradas da camada\n",
    "\n",
    "        self.lastLayer = lastLayer\n",
    "\n",
    "\n",
    "    # Inicializa os pesos e bias da camada\n",
    "    def initLayer(self) -> None:\n",
    "        if self.W is None and self.inputs is not None:\n",
    "            # Inicializa os pesos e bias com distribuição uniforme\n",
    "            # entre -0.5 e 0.5\n",
    "            self.W = np.random.rand(self.inputs.shape[-1], self.n_neurons) - 0.5\n",
    "            # self.W = np.random.rand(self.shape[0], self.shape[1]) - 0.5\n",
    "            self.b = np.random.rand(self.n_neurons) - 0.5\n",
    "\n",
    "\n",
    "    # Realiza o forward da camada\n",
    "    # Processando a entrada e aplicando a função de ativação\n",
    "    def selfForward(self, inputs) -> None:\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.initLayer() # Inicializa os pesos e bias, caso n tenham sido inicializados\n",
    "\n",
    "        self.out = np.dot(self.inputs, self.W) + self.b\n",
    "\n",
    "        # Se for a última camada, não aplica a função de ativação\n",
    "        if not self.lastLayer:\n",
    "            self.actv = sigmoid(self.out)\n",
    "        else:\n",
    "            self.actv = self.out\n",
    "\n",
    "\n",
    "    # Retorna a saída da camada (Z)\n",
    "    def getOutput(self):\n",
    "        return self.out\n",
    "    \n",
    "    \n",
    "    # Retorna a saída da camada com a função de ativação (A)\n",
    "    def getActivation(self):\n",
    "        return self.actv\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "Esta classe permite a gestão do modelo, viabilizando adicionar camadas ao modelo,\n",
    "o forward, o backward, o treinamento com base em um conjunto de treinamento, a predição\n",
    "com base em um conjunto de testes e, ainda, permite obter a ativação da dimensão\n",
    "latente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    # Construtor da classe\n",
    "    def __init__(self) -> None:\n",
    "        self.layers = np.array([])\n",
    "\n",
    "        self.epochs = None\n",
    "        self.lr = None\n",
    "\n",
    "        self.inputs = None # X\n",
    "        self.targets = None # X\n",
    "\n",
    "        self.outputs = []\n",
    "        self.activations = [] # X_Hat\n",
    "        \n",
    "\n",
    "    # Realiza o forward do autoencoder\n",
    "    # Recebe o input e passa por todas as camadas\n",
    "    # de modo que a saída de uma camada seja a entrada da próxima\n",
    "    def forward(self, inputs):\n",
    "        self.activations = [] # Limpa as ativações\n",
    "        self.outputs = [] # Limpa as saídas\n",
    "\n",
    "        self.activations.append(inputs)\n",
    "        self.outputs.append(inputs)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.selfForward(inputs)\n",
    "            inputs = layer.getActivation() # Saída da camada é a entrada da próxima\n",
    "            output = layer.getOutput()\n",
    "\n",
    "            self.outputs.append(output) # Salva a saída da camada\n",
    "            self.activations.append(inputs) # Salva a saída com a função de ativação\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    # Realiza o backward do autoencoder\n",
    "    def backward(self) -> None:\n",
    "        i = self.layers.size # Número de camadas\n",
    "        n = self.inputs.shape[0] # Número de amostras\n",
    "\n",
    "        err = 2 * (self.targets - self.activations[-1]) # 2 * (x - x_hat)\n",
    "\n",
    "        # Para cada layer e output(z), calcula o delta e atualiza os pesos e bias\n",
    "        for layer, z in zip(self.layers[::-1], self.outputs[::-1]):\n",
    "            delta = err\n",
    "\n",
    "            # Não calcula a derivada ativação da última camada\n",
    "            if (i != self.layers.size):\n",
    "                # Calcula o delta para a próxima iteração\n",
    "                delta *= sigmoid(z, derivative=True)\n",
    "\n",
    "            err = np.dot(delta, layer.W.T) # Atualiza o erro para a prox iteracao\n",
    "\n",
    "            if i > 0: # Evita index out of range\n",
    "                prev_act = self.activations[i-1] # Ativação da camada anterior\n",
    "                dW = np.dot(prev_act.T, delta) / n\n",
    "                db = delta.mean()\n",
    "\n",
    "                layer.W += self.lr * dW\n",
    "                layer.b += self.lr * db\n",
    "\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        \n",
    "    # Treina o modelo, realizando o forward e backward epochs vezes\n",
    "    # e printando o erro quadrático médio a cada 10 épocas\n",
    "    def fit(self, inputs, targets, lr: float = 0.01, epochs: int = 100) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.lr = lr\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(inputs)\n",
    "            self.backward()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch: {epoch}')\n",
    "                print(f'Loss: {np.mean(np.square(self.targets - self.activations[-1]))}')\n",
    "\n",
    "\n",
    "    # Adiciona uma camada ao autoencoder\n",
    "    def addLayer(self, layer: Layer) -> None:\n",
    "        self.layers = np.append(self.layers, layer)\n",
    "\n",
    "\n",
    "    # Retorna a predição do input (X_hat) com base no modelo treinado\n",
    "    def predict(self, inputs) -> np.ndarray:\n",
    "        self.forward(inputs)\n",
    "        return self.activations[-1]\n",
    "    \n",
    "    \n",
    "    # Retorna a ativação da dimensão latente\n",
    "    def getLatentSpace(self) -> np.ndarray:\n",
    "        return self.layers[self.layers.size // 2 - 1].getActivation()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do modelo\n",
    "Esta função instancia a classe autoencoder.\n",
    "\n",
    "Ela recebe a dimensão dos atributos de entrada, as camadas ocultas (na parte do\n",
    "encoder) e a função de ativação.\n",
    "\n",
    "Esta função irá instanciar os layers de forma que seja espelhado, ou seja:\n",
    "\n",
    "Se inputDim = 10, e as camadas ocultas têm os tamanhos layers=[5, 3], então\n",
    "a camada com 3 neurônios será a dimensão latente, sendo tudo antes dela espelhado.\n",
    "\n",
    "Assim, ficará no formato: [10, 5, 3, 5, 10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(inputDim, layers):\n",
    "    model = Autoencoder()\n",
    "\n",
    "    # Adiciona as camadas do encoder\n",
    "    for i in range(len(layers)):\n",
    "        model.addLayer(Layer(layers[i]))\n",
    "\n",
    "    # Adiciona as camadas do decoder\n",
    "    for i in range(len(layers)-2, -1, -1):\n",
    "        model.addLayer(Layer(layers[i]))\n",
    "\n",
    "    # Adiciona a camada de saída\n",
    "    model.addLayer(Layer(inputDim, lastLayer = True))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, os dados serão gerados e pré-processados, criando-se modelo\n",
    "e definindo a arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 1.0869104801430287\n",
      "Epoch: 10\n",
      "Loss: 0.20429825788712913\n",
      "Epoch: 20\n",
      "Loss: 0.08087524324377922\n",
      "Epoch: 30\n",
      "Loss: 0.05945896150788768\n",
      "Epoch: 40\n",
      "Loss: 0.05571821272817437\n",
      "Epoch: 50\n",
      "Loss: 0.05503974844952127\n",
      "Epoch: 60\n",
      "Loss: 0.05489724153298014\n",
      "Epoch: 70\n",
      "Loss: 0.054850797933948404\n",
      "Epoch: 80\n",
      "Loss: 0.054822125442479074\n",
      "Epoch: 90\n",
      "Loss: 0.054796961427108735\n",
      "Epoch: 100\n",
      "Loss: 0.05477265021264395\n",
      "Epoch: 110\n",
      "Loss: 0.05474868102894885\n",
      "Epoch: 120\n",
      "Loss: 0.05472494532952147\n",
      "Epoch: 130\n",
      "Loss: 0.05470141097565309\n",
      "Epoch: 140\n",
      "Loss: 0.054678060653408966\n",
      "Epoch: 150\n",
      "Loss: 0.05465488013466582\n",
      "Epoch: 160\n",
      "Loss: 0.05463185601814948\n",
      "Epoch: 170\n",
      "Loss: 0.0546089752844451\n",
      "Epoch: 180\n",
      "Loss: 0.05458622519982417\n",
      "Epoch: 190\n",
      "Loss: 0.05456359328756695\n",
      "Epoch: 200\n",
      "Loss: 0.054541067312580495\n",
      "Epoch: 210\n",
      "Loss: 0.05451863526884084\n",
      "Epoch: 220\n",
      "Loss: 0.05449628536762815\n",
      "Epoch: 230\n",
      "Loss: 0.05447400602615512\n",
      "Epoch: 240\n",
      "Loss: 0.054451785856503204\n",
      "Epoch: 250\n",
      "Loss: 0.05442961365484246\n",
      "Epoch: 260\n",
      "Loss: 0.054407478390921686\n",
      "Epoch: 270\n",
      "Loss: 0.05438536919781788\n",
      "Epoch: 280\n",
      "Loss: 0.05436327536193407\n",
      "Epoch: 290\n",
      "Loss: 0.054341186313234875\n",
      "Epoch: 300\n",
      "Loss: 0.0543190916157087\n",
      "Epoch: 310\n",
      "Loss: 0.05429698095804585\n",
      "Epoch: 320\n",
      "Loss: 0.054274844144521696\n",
      "Epoch: 330\n",
      "Loss: 0.05425267108607392\n",
      "Epoch: 340\n",
      "Loss: 0.054230451791563515\n",
      "Epoch: 350\n",
      "Loss: 0.05420817635920843\n",
      "Epoch: 360\n",
      "Loss: 0.05418583496817975\n",
      "Epoch: 370\n",
      "Loss: 0.054163417870349735\n",
      "Epoch: 380\n",
      "Loss: 0.05414091538218167\n",
      "Epoch: 390\n",
      "Loss: 0.054118317876751094\n",
      "Epoch: 400\n",
      "Loss: 0.05409561577588881\n",
      "Epoch: 410\n",
      "Loss: 0.05407279954243538\n",
      "Epoch: 420\n",
      "Loss: 0.05404985967259774\n",
      "Epoch: 430\n",
      "Loss: 0.05402678668839808\n",
      "Epoch: 440\n",
      "Loss: 0.054003571130205805\n",
      "Epoch: 450\n",
      "Loss: 0.05398020354934299\n",
      "Epoch: 460\n",
      "Loss: 0.053956674500754434\n",
      "Epoch: 470\n",
      "Loss: 0.05393297453573297\n",
      "Epoch: 480\n",
      "Loss: 0.053909094194691434\n",
      "Epoch: 490\n",
      "Loss: 0.05388502399997211\n",
      "Epoch: 500\n",
      "Loss: 0.05386075444868532\n",
      "Epoch: 510\n",
      "Loss: 0.05383627600556829\n",
      "Epoch: 520\n",
      "Loss: 0.05381157909585594\n",
      "Epoch: 530\n",
      "Loss: 0.053786654098155154\n",
      "Epoch: 540\n",
      "Loss: 0.053761491337314234\n",
      "Epoch: 550\n",
      "Loss: 0.05373608107727932\n",
      "Epoch: 560\n",
      "Loss: 0.05371041351392956\n",
      "Epoch: 570\n",
      "Loss: 0.053684478767883084\n",
      "Epoch: 580\n",
      "Loss: 0.05365826687726563\n",
      "Epoch: 590\n",
      "Loss: 0.053631767790433986\n",
      "Epoch: 600\n",
      "Loss: 0.053604971358646204\n",
      "Epoch: 610\n",
      "Loss: 0.053577867328671015\n",
      "Epoch: 620\n",
      "Loss: 0.053550445335328295\n",
      "Epoch: 630\n",
      "Loss: 0.05352269489395322\n",
      "Epoch: 640\n",
      "Loss: 0.05349460539277606\n",
      "Epoch: 650\n",
      "Loss: 0.053466166085210265\n",
      "Epoch: 660\n",
      "Loss: 0.05343736608204094\n",
      "Epoch: 670\n",
      "Loss: 0.05340819434350639\n",
      "Epoch: 680\n",
      "Loss: 0.053378639671265\n",
      "Epoch: 690\n",
      "Loss: 0.05334869070024002\n",
      "Epoch: 700\n",
      "Loss: 0.05331833589033484\n",
      "Epoch: 710\n",
      "Loss: 0.05328756351801119\n",
      "Epoch: 720\n",
      "Loss: 0.05325636166772303\n",
      "Epoch: 730\n",
      "Loss: 0.053224718223198665\n",
      "Epoch: 740\n",
      "Loss: 0.053192620858563924\n",
      "Epoch: 750\n",
      "Loss: 0.05316005702929891\n",
      "Epoch: 760\n",
      "Loss: 0.05312701396302141\n",
      "Epoch: 770\n",
      "Loss: 0.05309347865008971\n",
      "Epoch: 780\n",
      "Loss: 0.05305943783401754\n",
      "Epoch: 790\n",
      "Loss: 0.05302487800169455\n",
      "Epoch: 800\n",
      "Loss: 0.052989785373405006\n",
      "Epoch: 810\n",
      "Loss: 0.052954145892637965\n",
      "Epoch: 820\n",
      "Loss: 0.05291794521568235\n",
      "Epoch: 830\n",
      "Loss: 0.05288116870099988\n",
      "Epoch: 840\n",
      "Loss: 0.0528438013983697\n",
      "Epoch: 850\n",
      "Loss: 0.05280582803779783\n",
      "Epoch: 860\n",
      "Loss: 0.0527672330181855\n",
      "Epoch: 870\n",
      "Loss: 0.0527280003957498\n",
      "Epoch: 880\n",
      "Loss: 0.052688113872190814\n",
      "Epoch: 890\n",
      "Loss: 0.052647556782599164\n",
      "Epoch: 900\n",
      "Loss: 0.052606312083098225\n",
      "Epoch: 910\n",
      "Loss: 0.05256436233821541\n",
      "Epoch: 920\n",
      "Loss: 0.052521689707976986\n",
      "Epoch: 930\n",
      "Loss: 0.05247827593472124\n",
      "Epoch: 940\n",
      "Loss: 0.05243410232962485\n",
      "Epoch: 950\n",
      "Loss: 0.052389149758937506\n",
      "Epoch: 960\n",
      "Loss: 0.05234339862992018\n",
      "Epoch: 970\n",
      "Loss: 0.05229682887648239\n",
      "Epoch: 980\n",
      "Loss: 0.05224941994451437\n",
      "Epoch: 990\n",
      "Loss: 0.0522011507769097\n",
      "Epoch: 1000\n",
      "Loss: 0.052151999798274964\n",
      "Epoch: 1010\n",
      "Loss: 0.05210194489932237\n",
      "Epoch: 1020\n",
      "Loss: 0.05205096342094221\n",
      "Epoch: 1030\n",
      "Loss: 0.05199903213795166\n",
      "Epoch: 1040\n",
      "Loss: 0.051946127242517195\n",
      "Epoch: 1050\n",
      "Loss: 0.05189222432724769\n",
      "Epoch: 1060\n",
      "Loss: 0.05183729836795576\n",
      "Epoch: 1070\n",
      "Loss: 0.051781323706084965\n",
      "Epoch: 1080\n",
      "Loss: 0.05172427403080102\n",
      "Epoch: 1090\n",
      "Loss: 0.05166612236074498\n",
      "Epoch: 1100\n",
      "Loss: 0.05160684102544708\n",
      "Epoch: 1110\n",
      "Loss: 0.051546401646399904\n",
      "Epoch: 1120\n",
      "Loss: 0.05148477511778996\n",
      "Epoch: 1130\n",
      "Loss: 0.051421931586887064\n",
      "Epoch: 1140\n",
      "Loss: 0.051357840434091355\n",
      "Epoch: 1150\n",
      "Loss: 0.051292470252638056\n",
      "Epoch: 1160\n",
      "Loss: 0.051225788827960715\n",
      "Epoch: 1170\n",
      "Loss: 0.05115776311671419\n",
      "Epoch: 1180\n",
      "Loss: 0.05108835922545909\n",
      "Epoch: 1190\n",
      "Loss: 0.051017542389010566\n",
      "Epoch: 1200\n",
      "Loss: 0.050945276948454604\n",
      "Epoch: 1210\n",
      "Loss: 0.0508715263288367\n",
      "Epoch: 1220\n",
      "Loss: 0.05079625301652847\n",
      "Epoch: 1230\n",
      "Loss: 0.050719418536279454\n",
      "Epoch: 1240\n",
      "Loss: 0.050640983427962846\n",
      "Epoch: 1250\n",
      "Loss: 0.050560907223026356\n",
      "Epoch: 1260\n",
      "Loss: 0.05047914842066081\n",
      "Epoch: 1270\n",
      "Loss: 0.05039566446370276\n",
      "Epoch: 1280\n",
      "Loss: 0.05031041171428961\n",
      "Epoch: 1290\n",
      "Loss: 0.050223345429289995\n",
      "Epoch: 1300\n",
      "Loss: 0.05013441973553578\n",
      "Epoch: 1310\n",
      "Loss: 0.05004358760488695\n",
      "Epoch: 1320\n",
      "Loss: 0.04995080082916596\n",
      "Epoch: 1330\n",
      "Loss: 0.04985600999500428\n",
      "Epoch: 1340\n",
      "Loss: 0.049759164458650376\n",
      "Epoch: 1350\n",
      "Loss: 0.0496602123207967\n",
      "Epoch: 1360\n",
      "Loss: 0.04955910040149125\n",
      "Epoch: 1370\n",
      "Loss: 0.049455774215209435\n",
      "Epoch: 1380\n",
      "Loss: 0.049350177946172476\n",
      "Epoch: 1390\n",
      "Loss: 0.04924225442401058\n",
      "Epoch: 1400\n",
      "Loss: 0.04913194509988243\n",
      "Epoch: 1410\n",
      "Loss: 0.04901919002317657\n",
      "Epoch: 1420\n",
      "Loss: 0.04890392781893681\n",
      "Epoch: 1430\n",
      "Loss: 0.04878609566617049\n",
      "Epoch: 1440\n",
      "Loss: 0.048665629277217744\n",
      "Epoch: 1450\n",
      "Loss: 0.048542462878379974\n",
      "Epoch: 1460\n",
      "Loss: 0.048416529192028296\n",
      "Epoch: 1470\n",
      "Loss: 0.048287759420435845\n",
      "Epoch: 1480\n",
      "Loss: 0.04815608323160435\n",
      "Epoch: 1490\n",
      "Loss: 0.0480214287473814\n",
      "Epoch: 1500\n",
      "Loss: 0.04788372253419508\n",
      "Epoch: 1510\n",
      "Loss: 0.04774288959676275\n",
      "Epoch: 1520\n",
      "Loss: 0.047598853375163785\n",
      "Epoch: 1530\n",
      "Loss: 0.04745153574570042\n",
      "Epoch: 1540\n",
      "Loss: 0.04730085702600696\n",
      "Epoch: 1550\n",
      "Loss: 0.047146735984905576\n",
      "Epoch: 1560\n",
      "Loss: 0.0469890898575463\n",
      "Epoch: 1570\n",
      "Loss: 0.046827834366409776\n",
      "Epoch: 1580\n",
      "Loss: 0.04666288374879351\n",
      "Epoch: 1590\n",
      "Loss: 0.04649415079144608\n",
      "Epoch: 1600\n",
      "Loss: 0.04632154687305783\n",
      "Epoch: 1610\n",
      "Loss: 0.04614498201536219\n",
      "Epoch: 1620\n",
      "Loss: 0.04596436494364679\n",
      "Epoch: 1630\n",
      "Loss: 0.04577960315751997\n",
      "Epoch: 1640\n",
      "Loss: 0.045590603012822604\n",
      "Epoch: 1650\n",
      "Loss: 0.04539726981562079\n",
      "Epoch: 1660\n",
      "Loss: 0.04519950792925722\n",
      "Epoch: 1670\n",
      "Loss: 0.04499722089548076\n",
      "Epoch: 1680\n",
      "Loss: 0.044790311570711845\n",
      "Epoch: 1690\n",
      "Loss: 0.04457868227853663\n",
      "Epoch: 1700\n",
      "Loss: 0.044362234979553\n",
      "Epoch: 1710\n",
      "Loss: 0.04414087145971649\n",
      "Epoch: 1720\n",
      "Loss: 0.04391449353835299\n",
      "Epoch: 1730\n",
      "Loss: 0.0436830032970149\n",
      "Epoch: 1740\n",
      "Loss: 0.04344630333035911\n",
      "Epoch: 1750\n",
      "Loss: 0.043204297020215235\n",
      "Epoch: 1760\n",
      "Loss: 0.042956888833990375\n",
      "Epoch: 1770\n",
      "Loss: 0.04270398464852004\n",
      "Epoch: 1780\n",
      "Loss: 0.04244549210042266\n",
      "Epoch: 1790\n",
      "Loss: 0.042181320963943524\n",
      "Epoch: 1800\n",
      "Loss: 0.04191138355718348\n",
      "Epoch: 1810\n",
      "Loss: 0.04163559517749323\n",
      "Epoch: 1820\n",
      "Loss: 0.0413538745666763\n",
      "Epoch: 1830\n",
      "Loss: 0.0410661444064777\n",
      "Epoch: 1840\n",
      "Loss: 0.04077233184464073\n",
      "Epoch: 1850\n",
      "Loss: 0.04047236905158898\n",
      "Epoch: 1860\n",
      "Loss: 0.040166193807530066\n",
      "Epoch: 1870\n",
      "Loss: 0.039853750119484804\n",
      "Epoch: 1880\n",
      "Loss: 0.03953498886741351\n",
      "Epoch: 1890\n",
      "Loss: 0.039209868478243266\n",
      "Epoch: 1900\n",
      "Loss: 0.03887835562619346\n",
      "Epoch: 1910\n",
      "Loss: 0.03854042595735166\n",
      "Epoch: 1920\n",
      "Loss: 0.03819606483597009\n",
      "Epoch: 1930\n",
      "Loss: 0.037845268109434116\n",
      "Epoch: 1940\n",
      "Loss: 0.03748804288830227\n",
      "Epoch: 1950\n",
      "Loss: 0.0371244083372352\n",
      "Epoch: 1960\n",
      "Loss: 0.036754396472022714\n",
      "Epoch: 1970\n",
      "Loss: 0.03637805295729047\n",
      "Epoch: 1980\n",
      "Loss: 0.035995437898827705\n",
      "Epoch: 1990\n",
      "Loss: 0.03560662662383208\n",
      "Epoch: 2000\n",
      "Loss: 0.03521171044172997\n",
      "Epoch: 2010\n",
      "Loss: 0.034810797377608484\n",
      "Epoch: 2020\n",
      "Loss: 0.034404012869704084\n",
      "Epoch: 2030\n",
      "Loss: 0.033991500421846246\n",
      "Epoch: 2040\n",
      "Loss: 0.033573422201266793\n",
      "Epoch: 2050\n",
      "Loss: 0.03314995957177452\n",
      "Epoch: 2060\n",
      "Loss: 0.032721313551977334\n",
      "Epoch: 2070\n",
      "Loss: 0.0322877051880273\n",
      "Epoch: 2080\n",
      "Loss: 0.03184937583028732\n",
      "Epoch: 2090\n",
      "Loss: 0.03140658730338715\n",
      "Epoch: 2100\n",
      "Loss: 0.03095962195937061\n",
      "Epoch: 2110\n",
      "Loss: 0.03050878260404788\n",
      "Epoch: 2120\n",
      "Loss: 0.030054392287272823\n",
      "Epoch: 2130\n",
      "Loss: 0.029596793948674715\n",
      "Epoch: 2140\n",
      "Loss: 0.029136349911395222\n",
      "Epoch: 2150\n",
      "Loss: 0.028673441217618403\n",
      "Epoch: 2160\n",
      "Loss: 0.028208466801133925\n",
      "Epoch: 2170\n",
      "Loss: 0.02774184249383453\n",
      "Epoch: 2180\n",
      "Loss: 0.02727399986490832\n",
      "Epoch: 2190\n",
      "Loss: 0.026805384893525252\n",
      "Epoch: 2200\n",
      "Loss: 0.02633645647801303\n",
      "Epoch: 2210\n",
      "Loss: 0.02586768478683917\n",
      "Epoch: 2220\n",
      "Loss: 0.02539954945912708\n",
      "Epoch: 2230\n",
      "Loss: 0.02493253766489285\n",
      "Epoch: 2240\n",
      "Loss: 0.02446714203764746\n",
      "Epoch: 2250\n",
      "Loss: 0.02400385849441571\n",
      "Epoch: 2260\n",
      "Loss: 0.02354318396052235\n",
      "Epoch: 2270\n",
      "Loss: 0.023085614018632246\n",
      "Epoch: 2280\n",
      "Loss: 0.02263164050344668\n",
      "Epoch: 2290\n",
      "Loss: 0.022181749065099274\n",
      "Epoch: 2300\n",
      "Loss: 0.021736416725608707\n",
      "Epoch: 2310\n",
      "Loss: 0.02129610945368788\n",
      "Epoch: 2320\n",
      "Loss: 0.02086127978373992\n",
      "Epoch: 2330\n",
      "Loss: 0.020432364504962438\n",
      "Epoch: 2340\n",
      "Loss: 0.02000978244611271\n",
      "Epoch: 2350\n",
      "Loss: 0.01959393238065118\n",
      "Epoch: 2360\n",
      "Loss: 0.019185191075683332\n",
      "Epoch: 2370\n",
      "Loss: 0.0187839115063788\n",
      "Epoch: 2380\n",
      "Loss: 0.018390421255391427\n",
      "Epoch: 2390\n",
      "Loss: 0.018005021114276838\n",
      "Epoch: 2400\n",
      "Loss: 0.017627983901058342\n",
      "Epoch: 2410\n",
      "Loss: 0.017259553504989016\n",
      "Epoch: 2420\n",
      "Loss: 0.016899944166267493\n",
      "Epoch: 2430\n",
      "Loss: 0.016549339995061342\n",
      "Epoch: 2440\n",
      "Loss: 0.01620789473075181\n",
      "Epoch: 2450\n",
      "Loss: 0.015875731738914798\n",
      "Epoch: 2460\n",
      "Loss: 0.015552944240268825\n",
      "Epoch: 2470\n",
      "Loss: 0.015239595762722635\n",
      "Epoch: 2480\n",
      "Loss: 0.014935720804804957\n",
      "Epoch: 2490\n",
      "Loss: 0.014641325696211896\n",
      "Epoch: 2500\n",
      "Loss: 0.014356389639007906\n",
      "Epoch: 2510\n",
      "Loss: 0.014080865911197005\n",
      "Epoch: 2520\n",
      "Loss: 0.013814683212964597\n",
      "Epoch: 2530\n",
      "Loss: 0.013557747134886378\n",
      "Epoch: 2540\n",
      "Loss: 0.013309941726808612\n",
      "Epoch: 2550\n",
      "Loss: 0.01307113114591194\n",
      "Epoch: 2560\n",
      "Loss: 0.012841161362657413\n",
      "Epoch: 2570\n",
      "Loss: 0.012619861903849812\n",
      "Epoch: 2580\n",
      "Loss: 0.012407047612903115\n",
      "Epoch: 2590\n",
      "Loss: 0.012202520408515473\n",
      "Epoch: 2600\n",
      "Loss: 0.012006071024311974\n",
      "Epoch: 2610\n",
      "Loss: 0.011817480713546206\n",
      "Epoch: 2620\n",
      "Loss: 0.011636522904620169\n",
      "Epoch: 2630\n",
      "Loss: 0.011462964794940803\n",
      "Epoch: 2640\n",
      "Loss: 0.011296568872436956\n",
      "Epoch: 2650\n",
      "Loss: 0.011137094355873739\n",
      "Epoch: 2660\n",
      "Loss: 0.010984298546885008\n",
      "Epoch: 2670\n",
      "Loss: 0.010837938088369022\n",
      "Epoch: 2680\n",
      "Loss: 0.0106977701255294\n",
      "Epoch: 2690\n",
      "Loss: 0.010563553367372717\n",
      "Epoch: 2700\n",
      "Loss: 0.01043504904787858\n",
      "Epoch: 2710\n",
      "Loss: 0.010312021787325735\n",
      "Epoch: 2720\n",
      "Loss: 0.010194240355381929\n",
      "Epoch: 2730\n",
      "Loss: 0.010081478338541795\n",
      "Epoch: 2740\n",
      "Loss: 0.009973514715327167\n",
      "Epoch: 2750\n",
      "Loss: 0.009870134343350349\n",
      "Epoch: 2760\n",
      "Loss: 0.009771128362889792\n",
      "Epoch: 2770\n",
      "Loss: 0.009676294522046602\n",
      "Epoch: 2780\n",
      "Loss: 0.009585437428849503\n",
      "Epoch: 2790\n",
      "Loss: 0.009498368735865377\n",
      "Epoch: 2800\n",
      "Loss: 0.009414907262964057\n",
      "Epoch: 2810\n",
      "Loss: 0.009334879063891056\n",
      "Epoch: 2820\n",
      "Loss: 0.009258117442231985\n",
      "Epoch: 2830\n",
      "Loss: 0.009184462922219026\n",
      "Epoch: 2840\n",
      "Loss: 0.009113763179643842\n",
      "Epoch: 2850\n",
      "Loss: 0.009045872937912614\n",
      "Epoch: 2860\n",
      "Loss: 0.008980653834017657\n",
      "Epoch: 2870\n",
      "Loss: 0.008917974258913915\n",
      "Epoch: 2880\n",
      "Loss: 0.0088577091764864\n",
      "Epoch: 2890\n",
      "Loss: 0.008799739924982028\n",
      "Epoch: 2900\n",
      "Loss: 0.008743954004463166\n",
      "Epoch: 2910\n",
      "Loss: 0.008690244853525037\n",
      "Epoch: 2920\n",
      "Loss: 0.008638511618209327\n",
      "Epoch: 2930\n",
      "Loss: 0.00858865891574548\n",
      "Epoch: 2940\n",
      "Loss: 0.008540596595461946\n",
      "Epoch: 2950\n",
      "Loss: 0.008494239498934101\n",
      "Epoch: 2960\n",
      "Loss: 0.00844950722117554\n",
      "Epoch: 2970\n",
      "Loss: 0.008406323874435828\n",
      "Epoch: 2980\n",
      "Loss: 0.008364617855941262\n",
      "Epoch: 2990\n",
      "Loss: 0.00832432162070613\n",
      "Epoch: 3000\n",
      "Loss: 0.008285371460350357\n",
      "Epoch: 3010\n",
      "Loss: 0.008247707288684892\n",
      "Epoch: 3020\n",
      "Loss: 0.008211272434668376\n",
      "Epoch: 3030\n",
      "Loss: 0.008176013443196944\n",
      "Epoch: 3040\n",
      "Loss: 0.008141879884062514\n",
      "Epoch: 3050\n",
      "Loss: 0.008108824169303103\n",
      "Epoch: 3060\n",
      "Loss: 0.008076801379070254\n",
      "Epoch: 3070\n",
      "Loss: 0.008045769096053052\n",
      "Epoch: 3080\n",
      "Loss: 0.008015687248424257\n",
      "Epoch: 3090\n",
      "Loss: 0.007986517961210861\n",
      "Epoch: 3100\n",
      "Loss: 0.007958225415938113\n",
      "Epoch: 3110\n",
      "Loss: 0.00793077571835166\n",
      "Epoch: 3120\n",
      "Loss: 0.007904136773986199\n",
      "Epoch: 3130\n",
      "Loss: 0.00787827817131998\n",
      "Epoch: 3140\n",
      "Loss: 0.007853171072232048\n",
      "Epoch: 3150\n",
      "Loss: 0.00782878810946222\n",
      "Epoch: 3160\n",
      "Loss: 0.007805103290762245\n",
      "Epoch: 3170\n",
      "Loss: 0.007782091909419099\n",
      "Epoch: 3180\n",
      "Loss: 0.007759730460828109\n",
      "Epoch: 3190\n",
      "Loss: 0.007737996564793404\n",
      "Epoch: 3200\n",
      "Loss: 0.007716868893235838\n",
      "Epoch: 3210\n",
      "Loss: 0.007696327102993664\n",
      "Epoch: 3220\n",
      "Loss: 0.007676351773408225\n",
      "Epoch: 3230\n",
      "Loss: 0.007656924348395581\n",
      "Epoch: 3240\n",
      "Loss: 0.007638027082714957\n",
      "Epoch: 3250\n",
      "Loss: 0.007619642992155753\n",
      "Epoch: 3260\n",
      "Loss: 0.0076017558073766455\n",
      "Epoch: 3270\n",
      "Loss: 0.007584349931142428\n",
      "Epoch: 3280\n",
      "Loss: 0.0075674103987168305\n",
      "Epoch: 3290\n",
      "Loss: 0.007550922841182254\n",
      "Epoch: 3300\n",
      "Loss: 0.007534873451470086\n",
      "Epoch: 3310\n",
      "Loss: 0.007519248952897889\n",
      "Epoch: 3320\n",
      "Loss: 0.007504036570022196\n",
      "Epoch: 3330\n",
      "Loss: 0.0074892240016277666\n",
      "Epoch: 3340\n",
      "Loss: 0.007474799395685988\n",
      "Epoch: 3350\n",
      "Loss: 0.007460751326126421\n",
      "Epoch: 3360\n",
      "Loss: 0.007447068771276472\n",
      "Epoch: 3370\n",
      "Loss: 0.007433741093834617\n",
      "Epoch: 3380\n",
      "Loss: 0.007420758022252492\n",
      "Epoch: 3390\n",
      "Loss: 0.007408109633410654\n",
      "Epoch: 3400\n",
      "Loss: 0.007395786336481688\n",
      "Epoch: 3410\n",
      "Loss: 0.007383778857882715\n",
      "Epoch: 3420\n",
      "Loss: 0.007372078227227241\n",
      "Epoch: 3430\n",
      "Loss: 0.007360675764193618\n",
      "Epoch: 3440\n",
      "Loss: 0.007349563066234281\n",
      "Epoch: 3450\n",
      "Loss: 0.007338731997056284\n",
      "Epoch: 3460\n",
      "Loss: 0.007328174675809626\n",
      "Epoch: 3470\n",
      "Loss: 0.0073178834669252735\n",
      "Epoch: 3480\n",
      "Loss: 0.007307850970549958\n",
      "Epoch: 3490\n",
      "Loss: 0.007298070013529382\n",
      "Epoch: 3500\n",
      "Loss: 0.007288533640895886\n",
      "Epoch: 3510\n",
      "Loss: 0.0072792351078204455\n",
      "Epoch: 3520\n",
      "Loss: 0.0072701678719926145\n",
      "Epoch: 3530\n",
      "Loss: 0.0072613255863952135\n",
      "Epoch: 3540\n",
      "Loss: 0.0072527020924436775\n",
      "Epoch: 3550\n",
      "Loss: 0.007244291413462661\n",
      "Epoch: 3560\n",
      "Loss: 0.007236087748475052\n",
      "Epoch: 3570\n",
      "Loss: 0.007228085466280798\n",
      "Epoch: 3580\n",
      "Loss: 0.00722027909980499\n",
      "Epoch: 3590\n",
      "Loss: 0.007212663340696598\n",
      "Epoch: 3600\n",
      "Loss: 0.007205233034160855\n",
      "Epoch: 3610\n",
      "Loss: 0.007197983174009905\n",
      "Epoch: 3620\n",
      "Loss: 0.0071909088979176345\n",
      "Epoch: 3630\n",
      "Loss: 0.0071840054828659515\n",
      "Epoch: 3640\n",
      "Loss: 0.00717726834077081\n",
      "Epoch: 3650\n",
      "Loss: 0.007170693014277367\n",
      "Epoch: 3660\n",
      "Loss: 0.007164275172714553\n",
      "Epoch: 3670\n",
      "Loss: 0.007158010608200131\n",
      "Epoch: 3680\n",
      "Loss: 0.00715189523188812\n",
      "Epoch: 3690\n",
      "Loss: 0.007145925070351096\n",
      "Epoch: 3700\n",
      "Loss: 0.0071400962620904685\n",
      "Epoch: 3710\n",
      "Loss: 0.007134405054168414\n",
      "Epoch: 3720\n",
      "Loss: 0.007128847798955605\n",
      "Epoch: 3730\n",
      "Loss: 0.007123420950989321\n",
      "Epoch: 3740\n",
      "Loss: 0.007118121063936931\n",
      "Epoch: 3750\n",
      "Loss: 0.007112944787660084\n",
      "Epoch: 3760\n",
      "Loss: 0.0071078888653752685\n",
      "Epoch: 3770\n",
      "Loss: 0.007102950130906699\n",
      "Epoch: 3780\n",
      "Loss: 0.007098125506027739\n",
      "Epoch: 3790\n",
      "Loss: 0.007093411997887323\n",
      "Epoch: 3800\n",
      "Loss: 0.007088806696518047\n",
      "Epoch: 3810\n",
      "Loss: 0.007084306772422788\n",
      "Epoch: 3820\n",
      "Loss: 0.0070799094742368986\n",
      "Epoch: 3830\n",
      "Loss: 0.007075612126463208\n",
      "Epoch: 3840\n",
      "Loss: 0.007071412127277142\n",
      "Epoch: 3850\n",
      "Loss: 0.007067306946399504\n",
      "Epoch: 3860\n",
      "Loss: 0.007063294123034503\n",
      "Epoch: 3870\n",
      "Loss: 0.0070593712638707855\n",
      "Epoch: 3880\n",
      "Loss: 0.007055536041143278\n",
      "Epoch: 3890\n",
      "Loss: 0.007051786190753825\n",
      "Epoch: 3900\n",
      "Loss: 0.007048119510448623\n",
      "Epoch: 3910\n",
      "Loss: 0.0070445338580505555\n",
      "Epoch: 3920\n",
      "Loss: 0.007041027149744656\n",
      "Epoch: 3930\n",
      "Loss: 0.007037597358414943\n",
      "Epoch: 3940\n",
      "Loss: 0.007034242512030961\n",
      "Epoch: 3950\n",
      "Loss: 0.007030960692082446\n",
      "Epoch: 3960\n",
      "Loss: 0.007027750032060561\n",
      "Epoch: 3970\n",
      "Loss: 0.007024608715984237\n",
      "Epoch: 3980\n",
      "Loss: 0.007021534976970179\n",
      "Epoch: 3990\n",
      "Loss: 0.007018527095845173\n",
      "Epoch: 4000\n",
      "Loss: 0.007015583399799372\n",
      "Epoch: 4010\n",
      "Loss: 0.00701270226107927\n",
      "Epoch: 4020\n",
      "Loss: 0.007009882095719148\n",
      "Epoch: 4030\n",
      "Loss: 0.007007121362309775\n",
      "Epoch: 4040\n",
      "Loss: 0.007004418560803258\n",
      "Epoch: 4050\n",
      "Loss: 0.007001772231352869\n",
      "Epoch: 4060\n",
      "Loss: 0.006999180953186829\n",
      "Epoch: 4070\n",
      "Loss: 0.006996643343514978\n",
      "Epoch: 4080\n",
      "Loss: 0.006994158056467333\n",
      "Epoch: 4090\n",
      "Loss: 0.0069917237820635745\n",
      "Epoch: 4100\n",
      "Loss: 0.006989339245212491\n",
      "Epoch: 4110\n",
      "Loss: 0.006987003204740511\n",
      "Epoch: 4120\n",
      "Loss: 0.006984714452448407\n",
      "Epoch: 4130\n",
      "Loss: 0.006982471812195338\n",
      "Epoch: 4140\n",
      "Loss: 0.006980274139009398\n",
      "Epoch: 4150\n",
      "Loss: 0.006978120318223872\n",
      "Epoch: 4160\n",
      "Loss: 0.00697600926463843\n",
      "Epoch: 4170\n",
      "Loss: 0.006973939921704501\n",
      "Epoch: 4180\n",
      "Loss: 0.006971911260734101\n",
      "Epoch: 4190\n",
      "Loss: 0.00696992228013143\n",
      "Epoch: 4200\n",
      "Loss: 0.00696797200464653\n",
      "Epoch: 4210\n",
      "Loss: 0.006966059484650359\n",
      "Epoch: 4220\n",
      "Loss: 0.006964183795430646\n",
      "Epoch: 4230\n",
      "Loss: 0.006962344036507893\n",
      "Epoch: 4240\n",
      "Loss: 0.006960539330970941\n",
      "Epoch: 4250\n",
      "Loss: 0.006958768824831491\n",
      "Epoch: 4260\n",
      "Loss: 0.006957031686397067\n",
      "Epoch: 4270\n",
      "Loss: 0.006955327105661801\n",
      "Epoch: 4280\n",
      "Loss: 0.006953654293714592\n",
      "Epoch: 4290\n",
      "Loss: 0.006952012482164054\n",
      "Epoch: 4300\n",
      "Loss: 0.0069504009225798144\n",
      "Epoch: 4310\n",
      "Loss: 0.0069488188859496295\n",
      "Epoch: 4320\n",
      "Loss: 0.006947265662151897\n",
      "Epoch: 4330\n",
      "Loss: 0.006945740559443077\n",
      "Epoch: 4340\n",
      "Loss: 0.006944242903959599\n",
      "Epoch: 4350\n",
      "Loss: 0.0069427720392338296\n",
      "Epoch: 4360\n",
      "Loss: 0.006941327325723681\n",
      "Epoch: 4370\n",
      "Loss: 0.006939908140355472\n",
      "Epoch: 4380\n",
      "Loss: 0.006938513876079633\n",
      "Epoch: 4390\n",
      "Loss: 0.006937143941438906\n",
      "Epoch: 4400\n",
      "Loss: 0.006935797760148656\n",
      "Epoch: 4410\n",
      "Loss: 0.006934474770688945\n",
      "Epoch: 4420\n",
      "Loss: 0.006933174425908029\n",
      "Epoch: 4430\n",
      "Loss: 0.006931896192636935\n",
      "Epoch: 4440\n",
      "Loss: 0.0069306395513148164\n",
      "Epoch: 4450\n",
      "Loss: 0.006929403995624762\n",
      "Epoch: 4460\n",
      "Loss: 0.006928189032139733\n",
      "Epoch: 4470\n",
      "Loss: 0.006926994179978387\n",
      "Epoch: 4480\n",
      "Loss: 0.006925818970470451\n",
      "Epoch: 4490\n",
      "Loss: 0.0069246629468313935\n",
      "Epoch: 4500\n",
      "Loss: 0.006923525663846134\n",
      "Epoch: 4510\n",
      "Loss: 0.0069224066875615015\n",
      "Epoch: 4520\n",
      "Loss: 0.006921305594987215\n",
      "Epoch: 4530\n",
      "Loss: 0.006920221973805132\n",
      "Epoch: 4540\n",
      "Loss: 0.00691915542208653\n",
      "Epoch: 4550\n",
      "Loss: 0.006918105548017184\n",
      "Epoch: 4560\n",
      "Loss: 0.006917071969630017\n",
      "Epoch: 4570\n",
      "Loss: 0.006916054314545122\n",
      "Epoch: 4580\n",
      "Loss: 0.006915052219716928\n",
      "Epoch: 4590\n",
      "Loss: 0.006914065331188301\n",
      "Epoch: 4600\n",
      "Loss: 0.006913093303851406\n",
      "Epoch: 4610\n",
      "Loss: 0.006912135801215119\n",
      "Epoch: 4620\n",
      "Loss: 0.0069111924951788025\n",
      "Epoch: 4630\n",
      "Loss: 0.00691026306581227\n",
      "Epoch: 4640\n",
      "Loss: 0.006909347201141769\n",
      "Epoch: 4650\n",
      "Loss: 0.006908444596941786\n",
      "Epoch: 4660\n",
      "Loss: 0.006907554956532545\n",
      "Epoch: 4670\n",
      "Loss: 0.0069066779905830175\n",
      "Epoch: 4680\n",
      "Loss: 0.006905813416919273\n",
      "Epoch: 4690\n",
      "Loss: 0.006904960960338061\n",
      "Epoch: 4700\n",
      "Loss: 0.006904120352425432\n",
      "Epoch: 4710\n",
      "Loss: 0.0069032913313802945\n",
      "Epoch: 4720\n",
      "Loss: 0.00690247364184273\n",
      "Epoch: 4730\n",
      "Loss: 0.006901667034726975\n",
      "Epoch: 4740\n",
      "Loss: 0.006900871267058893\n",
      "Epoch: 4750\n",
      "Loss: 0.006900086101817852\n",
      "Epoch: 4760\n",
      "Loss: 0.006899311307782861\n",
      "Epoch: 4770\n",
      "Loss: 0.006898546659382848\n",
      "Epoch: 4780\n",
      "Loss: 0.006897791936550966\n",
      "Epoch: 4790\n",
      "Loss: 0.006897046924582825\n",
      "Epoch: 4800\n",
      "Loss: 0.006896311413998515\n",
      "Epoch: 4810\n",
      "Loss: 0.006895585200408344\n",
      "Epoch: 4820\n",
      "Loss: 0.006894868084382168\n",
      "Epoch: 4830\n",
      "Loss: 0.006894159871322219\n",
      "Epoch: 4840\n",
      "Loss: 0.006893460371339325\n",
      "Epoch: 4850\n",
      "Loss: 0.006892769399132457\n",
      "Epoch: 4860\n",
      "Loss: 0.006892086773871464\n",
      "Epoch: 4870\n",
      "Loss: 0.006891412319082957\n",
      "Epoch: 4880\n",
      "Loss: 0.00689074586253921\n",
      "Epoch: 4890\n",
      "Loss: 0.006890087236150041\n",
      "Epoch: 4900\n",
      "Loss: 0.0068894362758575335\n",
      "Epoch: 4910\n",
      "Loss: 0.006888792821533583\n",
      "Epoch: 4920\n",
      "Loss: 0.0068881567168801415\n",
      "Epoch: 4930\n",
      "Loss: 0.006887527809332111\n",
      "Epoch: 4940\n",
      "Loss: 0.006886905949962805\n",
      "Epoch: 4950\n",
      "Loss: 0.006886290993391904\n",
      "Epoch: 4960\n",
      "Loss: 0.006885682797695854\n",
      "Epoch: 4970\n",
      "Loss: 0.00688508122432061\n",
      "Epoch: 4980\n",
      "Loss: 0.006884486137996698\n",
      "Epoch: 4990\n",
      "Loss: 0.006883897406656496\n",
      "Epoch: 5000\n",
      "Loss: 0.006883314901353698\n",
      "Epoch: 5010\n",
      "Loss: 0.006882738496184896\n",
      "Epoch: 5020\n",
      "Loss: 0.006882168068213212\n",
      "Epoch: 5030\n",
      "Loss: 0.006881603497393941\n",
      "Epoch: 5040\n",
      "Loss: 0.006881044666502124\n",
      "Epoch: 5050\n",
      "Loss: 0.0068804914610620434\n",
      "Epoch: 5060\n",
      "Loss: 0.006879943769278529\n",
      "Epoch: 5070\n",
      "Loss: 0.006879401481970083\n",
      "Epoch: 5080\n",
      "Loss: 0.00687886449250373\n",
      "Epoch: 5090\n",
      "Loss: 0.006878332696731584\n",
      "Epoch: 5100\n",
      "Loss: 0.006877805992929041\n",
      "Epoch: 5110\n",
      "Loss: 0.006877284281734595\n",
      "Epoch: 5120\n",
      "Loss: 0.006876767466091225\n",
      "Epoch: 5130\n",
      "Loss: 0.006876255451189276\n",
      "Epoch: 5140\n",
      "Loss: 0.0068757481444108445\n",
      "Epoch: 5150\n",
      "Loss: 0.0068752454552755875\n",
      "Epoch: 5160\n",
      "Loss: 0.006874747295387948\n",
      "Epoch: 5170\n",
      "Loss: 0.00687425357838573\n",
      "Epoch: 5180\n",
      "Loss: 0.006873764219890007\n",
      "Epoch: 5190\n",
      "Loss: 0.006873279137456326\n",
      "Epoch: 5200\n",
      "Loss: 0.006872798250527157\n",
      "Epoch: 5210\n",
      "Loss: 0.006872321480385578\n",
      "Epoch: 5220\n",
      "Loss: 0.006871848750110142\n",
      "Epoch: 5230\n",
      "Loss: 0.006871379984530907\n",
      "Epoch: 5240\n",
      "Loss: 0.006870915110186588\n",
      "Epoch: 5250\n",
      "Loss: 0.006870454055282821\n",
      "Epoch: 5260\n",
      "Loss: 0.006869996749651478\n",
      "Epoch: 5270\n",
      "Loss: 0.006869543124711027\n",
      "Epoch: 5280\n",
      "Loss: 0.006869093113427919\n",
      "Epoch: 5290\n",
      "Loss: 0.006868646650278935\n",
      "Epoch: 5300\n",
      "Loss: 0.006868203671214518\n",
      "Epoch: 5310\n",
      "Loss: 0.00686776411362302\n",
      "Epoch: 5320\n",
      "Loss: 0.006867327916295864\n",
      "Epoch: 5330\n",
      "Loss: 0.006866895019393604\n",
      "Epoch: 5340\n",
      "Loss: 0.006866465364412821\n",
      "Epoch: 5350\n",
      "Loss: 0.0068660388941538825\n",
      "Epoch: 5360\n",
      "Loss: 0.0068656155526895055\n",
      "Epoch: 5370\n",
      "Loss: 0.006865195285334119\n",
      "Epoch: 5380\n",
      "Loss: 0.006864778038614004\n",
      "Epoch: 5390\n",
      "Loss: 0.006864363760238177\n",
      "Epoch: 5400\n",
      "Loss: 0.0068639523990700205\n",
      "Epoch: 5410\n",
      "Loss: 0.006863543905099624\n",
      "Epoch: 5420\n",
      "Loss: 0.0068631382294168175\n",
      "Epoch: 5430\n",
      "Loss: 0.006862735324184884\n",
      "Epoch: 5440\n",
      "Loss: 0.006862335142614944\n",
      "Epoch: 5450\n",
      "Loss: 0.00686193763894096\n",
      "Epoch: 5460\n",
      "Loss: 0.006861542768395398\n",
      "Epoch: 5470\n",
      "Loss: 0.006861150487185459\n",
      "Epoch: 5480\n",
      "Loss: 0.006860760752469954\n",
      "Epoch: 5490\n",
      "Loss: 0.006860373522336705\n",
      "Epoch: 5500\n",
      "Loss: 0.006859988755780555\n",
      "Epoch: 5510\n",
      "Loss: 0.0068596064126819015\n",
      "Epoch: 5520\n",
      "Loss: 0.006859226453785763\n",
      "Epoch: 5530\n",
      "Loss: 0.0068588488406813855\n",
      "Epoch: 5540\n",
      "Loss: 0.006858473535782339\n",
      "Epoch: 5550\n",
      "Loss: 0.00685810050230711\n",
      "Epoch: 5560\n",
      "Loss: 0.006857729704260188\n",
      "Epoch: 5570\n",
      "Loss: 0.006857361106413606\n",
      "Epoch: 5580\n",
      "Loss: 0.006856994674288945\n",
      "Epoch: 5590\n",
      "Loss: 0.006856630374139781\n",
      "Epoch: 5600\n",
      "Loss: 0.006856268172934571\n",
      "Epoch: 5610\n",
      "Loss: 0.006855908038339956\n",
      "Epoch: 5620\n",
      "Loss: 0.006855549938704477\n",
      "Epoch: 5630\n",
      "Loss: 0.006855193843042686\n",
      "Epoch: 5640\n",
      "Loss: 0.006854839721019665\n",
      "Epoch: 5650\n",
      "Loss: 0.006854487542935906\n",
      "Epoch: 5660\n",
      "Loss: 0.0068541372797125705\n",
      "Epoch: 5670\n",
      "Loss: 0.00685378890287712\n",
      "Epoch: 5680\n",
      "Loss: 0.006853442384549273\n",
      "Epoch: 5690\n",
      "Loss: 0.006853097697427335\n",
      "Epoch: 5700\n",
      "Loss: 0.006852754814774842\n",
      "Epoch: 5710\n",
      "Loss: 0.0068524137104075365\n",
      "Epoch: 5720\n",
      "Loss: 0.006852074358680667\n",
      "Epoch: 5730\n",
      "Loss: 0.0068517367344765896\n",
      "Epoch: 5740\n",
      "Loss: 0.006851400813192667\n",
      "Epoch: 5750\n",
      "Loss: 0.006851066570729477\n",
      "Epoch: 5760\n",
      "Loss: 0.006850733983479294\n",
      "Epoch: 5770\n",
      "Loss: 0.006850403028314854\n",
      "Epoch: 5780\n",
      "Loss: 0.00685007368257839\n",
      "Epoch: 5790\n",
      "Loss: 0.0068497459240709395\n",
      "Epoch: 5800\n",
      "Loss: 0.006849419731041904\n",
      "Epoch: 5810\n",
      "Loss: 0.006849095082178861\n",
      "Epoch: 5820\n",
      "Loss: 0.0068487719565976215\n",
      "Epoch: 5830\n",
      "Loss: 0.006848450333832532\n",
      "Epoch: 5840\n",
      "Loss: 0.006848130193827008\n",
      "Epoch: 5850\n",
      "Loss: 0.006847811516924289\n",
      "Epoch: 5860\n",
      "Loss: 0.00684749428385842\n",
      "Epoch: 5870\n",
      "Loss: 0.006847178475745458\n",
      "Epoch: 5880\n",
      "Loss: 0.0068468640740748715\n",
      "Epoch: 5890\n",
      "Loss: 0.006846551060701157\n",
      "Epoch: 5900\n",
      "Loss: 0.006846239417835656\n",
      "Epoch: 5910\n",
      "Loss: 0.006845929128038569\n",
      "Epoch: 5920\n",
      "Loss: 0.0068456201742111466\n",
      "Epoch: 5930\n",
      "Loss: 0.006845312539588087\n",
      "Epoch: 5940\n",
      "Loss: 0.006845006207730098\n",
      "Epoch: 5950\n",
      "Loss: 0.006844701162516643\n",
      "Epoch: 5960\n",
      "Loss: 0.00684439738813886\n",
      "Epoch: 5970\n",
      "Loss: 0.006844094869092643\n",
      "Epoch: 5980\n",
      "Loss: 0.006843793590171895\n",
      "Epoch: 5990\n",
      "Loss: 0.0068434935364619365\n",
      "Epoch: 6000\n",
      "Loss: 0.006843194693333062\n",
      "Epoch: 6010\n",
      "Loss: 0.006842897046434272\n",
      "Epoch: 6020\n",
      "Loss: 0.006842600581687121\n",
      "Epoch: 6030\n",
      "Loss: 0.00684230528527974\n",
      "Epoch: 6040\n",
      "Loss: 0.0068420111436609775\n",
      "Epoch: 6050\n",
      "Loss: 0.00684171814353469\n",
      "Epoch: 6060\n",
      "Loss: 0.006841426271854166\n",
      "Epoch: 6070\n",
      "Loss: 0.00684113551581668\n",
      "Epoch: 6080\n",
      "Loss: 0.006840845862858163\n",
      "Epoch: 6090\n",
      "Loss: 0.006840557300648018\n",
      "Epoch: 6100\n",
      "Loss: 0.0068402698170840414\n",
      "Epoch: 6110\n",
      "Loss: 0.006839983400287467\n",
      "Epoch: 6120\n",
      "Loss: 0.006839698038598125\n",
      "Epoch: 6130\n",
      "Loss: 0.006839413720569715\n",
      "Epoch: 6140\n",
      "Loss: 0.006839130434965194\n",
      "Epoch: 6150\n",
      "Loss: 0.006838848170752259\n",
      "Epoch: 6160\n",
      "Loss: 0.006838566917098943\n",
      "Epoch: 6170\n",
      "Loss: 0.006838286663369314\n",
      "Epoch: 6180\n",
      "Loss: 0.006838007399119267\n",
      "Epoch: 6190\n",
      "Loss: 0.006837729114092425\n",
      "Epoch: 6200\n",
      "Loss: 0.0068374517982161135\n",
      "Epoch: 6210\n",
      "Loss: 0.0068371754415974565\n",
      "Epoch: 6220\n",
      "Loss: 0.0068369000345195365\n",
      "Epoch: 6230\n",
      "Loss: 0.00683662556743766\n",
      "Epoch: 6240\n",
      "Loss: 0.006836352030975695\n",
      "Epoch: 6250\n",
      "Loss: 0.006836079415922505\n",
      "Epoch: 6260\n",
      "Loss: 0.006835807713228463\n",
      "Epoch: 6270\n",
      "Loss: 0.00683553691400203\n",
      "Epoch: 6280\n",
      "Loss: 0.006835267009506437\n",
      "Epoch: 6290\n",
      "Loss: 0.006834997991156422\n",
      "Epoch: 6300\n",
      "Loss: 0.006834729850515048\n",
      "Epoch: 6310\n",
      "Loss: 0.006834462579290598\n",
      "Epoch: 6320\n",
      "Loss: 0.006834196169333532\n",
      "Epoch: 6330\n",
      "Loss: 0.006833930612633525\n",
      "Epoch: 6340\n",
      "Loss: 0.006833665901316554\n",
      "Epoch: 6350\n",
      "Loss: 0.006833402027642083\n",
      "Epoch: 6360\n",
      "Loss: 0.006833138984000259\n",
      "Epoch: 6370\n",
      "Loss: 0.006832876762909235\n",
      "Epoch: 6380\n",
      "Loss: 0.0068326153570125\n",
      "Epoch: 6390\n",
      "Loss: 0.006832354759076304\n",
      "Epoch: 6400\n",
      "Loss: 0.00683209496198712\n",
      "Epoch: 6410\n",
      "Loss: 0.006831835958749172\n",
      "Epoch: 6420\n",
      "Loss: 0.006831577742482023\n",
      "Epoch: 6430\n",
      "Loss: 0.006831320306418203\n",
      "Epoch: 6440\n",
      "Loss: 0.006831063643900907\n",
      "Epoch: 6450\n",
      "Loss: 0.00683080774838173\n",
      "Epoch: 6460\n",
      "Loss: 0.00683055261341846\n",
      "Epoch: 6470\n",
      "Loss: 0.006830298232672923\n",
      "Epoch: 6480\n",
      "Loss: 0.006830044599908867\n",
      "Epoch: 6490\n",
      "Loss: 0.0068297917089899015\n",
      "Epoch: 6500\n",
      "Loss: 0.006829539553877475\n",
      "Epoch: 6510\n",
      "Loss: 0.00682928812862891\n",
      "Epoch: 6520\n",
      "Loss: 0.00682903742739546\n",
      "Epoch: 6530\n",
      "Loss: 0.006828787444420438\n",
      "Epoch: 6540\n",
      "Loss: 0.006828538174037365\n",
      "Epoch: 6550\n",
      "Loss: 0.006828289610668157\n",
      "Epoch: 6560\n",
      "Loss: 0.006828041748821374\n",
      "Epoch: 6570\n",
      "Loss: 0.006827794583090487\n",
      "Epoch: 6580\n",
      "Loss: 0.006827548108152192\n",
      "Epoch: 6590\n",
      "Loss: 0.006827302318764758\n",
      "Epoch: 6600\n",
      "Loss: 0.006827057209766411\n",
      "Epoch: 6610\n",
      "Loss: 0.0068268127760737615\n",
      "Epoch: 6620\n",
      "Loss: 0.0068265690126802486\n",
      "Epoch: 6630\n",
      "Loss: 0.006826325914654642\n",
      "Epoch: 6640\n",
      "Loss: 0.006826083477139553\n",
      "Epoch: 6650\n",
      "Loss: 0.0068258416953499955\n",
      "Epoch: 6660\n",
      "Loss: 0.006825600564571963\n",
      "Epoch: 6670\n",
      "Loss: 0.0068253600801610635\n",
      "Epoch: 6680\n",
      "Loss: 0.006825120237541138\n",
      "Epoch: 6690\n",
      "Loss: 0.00682488103220296\n",
      "Epoch: 6700\n",
      "Loss: 0.006824642459702932\n",
      "Epoch: 6710\n",
      "Loss: 0.00682440451566181\n",
      "Epoch: 6720\n",
      "Loss: 0.006824167195763476\n",
      "Epoch: 6730\n",
      "Loss: 0.00682393049575371\n",
      "Epoch: 6740\n",
      "Loss: 0.006823694411439022\n",
      "Epoch: 6750\n",
      "Loss: 0.006823458938685465\n",
      "Epoch: 6760\n",
      "Loss: 0.006823224073417519\n",
      "Epoch: 6770\n",
      "Loss: 0.0068229898116169635\n",
      "Epoch: 6780\n",
      "Loss: 0.006822756149321799\n",
      "Epoch: 6790\n",
      "Loss: 0.006822523082625176\n",
      "Epoch: 6800\n",
      "Loss: 0.006822290607674347\n",
      "Epoch: 6810\n",
      "Loss: 0.00682205872066966\n",
      "Epoch: 6820\n",
      "Loss: 0.006821827417863539\n",
      "Epoch: 6830\n",
      "Loss: 0.006821596695559524\n",
      "Epoch: 6840\n",
      "Loss: 0.0068213665501113034\n",
      "Epoch: 6850\n",
      "Loss: 0.00682113697792178\n",
      "Epoch: 6860\n",
      "Loss: 0.0068209079754421535\n",
      "Epoch: 6870\n",
      "Loss: 0.006820679539171021\n",
      "Epoch: 6880\n",
      "Loss: 0.006820451665653498\n",
      "Epoch: 6890\n",
      "Loss: 0.006820224351480364\n",
      "Epoch: 6900\n",
      "Loss: 0.00681999759328721\n",
      "Epoch: 6910\n",
      "Loss: 0.006819771387753617\n",
      "Epoch: 6920\n",
      "Loss: 0.006819545731602353\n",
      "Epoch: 6930\n",
      "Loss: 0.0068193206215985775\n",
      "Epoch: 6940\n",
      "Loss: 0.006819096054549067\n",
      "Epoch: 6950\n",
      "Loss: 0.006818872027301459\n",
      "Epoch: 6960\n",
      "Loss: 0.006818648536743511\n",
      "Epoch: 6970\n",
      "Loss: 0.006818425579802366\n",
      "Epoch: 6980\n",
      "Loss: 0.006818203153443853\n",
      "Epoch: 6990\n",
      "Loss: 0.006817981254671776\n",
      "Epoch: 7000\n",
      "Loss: 0.006817759880527241\n",
      "Epoch: 7010\n",
      "Loss: 0.0068175390280879825\n",
      "Epoch: 7020\n",
      "Loss: 0.006817318694467713\n",
      "Epoch: 7030\n",
      "Loss: 0.006817098876815474\n",
      "Epoch: 7040\n",
      "Loss: 0.006816879572315017\n",
      "Epoch: 7050\n",
      "Loss: 0.0068166607781841825\n",
      "Epoch: 7060\n",
      "Loss: 0.006816442491674295\n",
      "Epoch: 7070\n",
      "Loss: 0.006816224710069581\n",
      "Epoch: 7080\n",
      "Loss: 0.006816007430686579\n",
      "Epoch: 7090\n",
      "Loss: 0.006815790650873584\n",
      "Epoch: 7100\n",
      "Loss: 0.006815574368010086\n",
      "Epoch: 7110\n",
      "Loss: 0.006815358579506227\n",
      "Epoch: 7120\n",
      "Loss: 0.006815143282802268\n",
      "Epoch: 7130\n",
      "Loss: 0.006814928475368068\n",
      "Epoch: 7140\n",
      "Loss: 0.006814714154702572\n",
      "Epoch: 7150\n",
      "Loss: 0.006814500318333314\n",
      "Epoch: 7160\n",
      "Loss: 0.006814286963815913\n",
      "Epoch: 7170\n",
      "Loss: 0.006814074088733605\n",
      "Epoch: 7180\n",
      "Loss: 0.006813861690696769\n",
      "Epoch: 7190\n",
      "Loss: 0.006813649767342459\n",
      "Epoch: 7200\n",
      "Loss: 0.0068134383163339605\n",
      "Epoch: 7210\n",
      "Loss: 0.006813227335360329\n",
      "Epoch: 7220\n",
      "Loss: 0.006813016822135979\n",
      "Epoch: 7230\n",
      "Loss: 0.006812806774400241\n",
      "Epoch: 7240\n",
      "Loss: 0.006812597189916945\n",
      "Epoch: 7250\n",
      "Loss: 0.006812388066474017\n",
      "Epoch: 7260\n",
      "Loss: 0.006812179401883074\n",
      "Epoch: 7270\n",
      "Loss: 0.00681197119397903\n",
      "Epoch: 7280\n",
      "Loss: 0.006811763440619711\n",
      "Epoch: 7290\n",
      "Loss: 0.006811556139685478\n",
      "Epoch: 7300\n",
      "Loss: 0.00681134928907885\n",
      "Epoch: 7310\n",
      "Loss: 0.0068111428867241525\n",
      "Epoch: 7320\n",
      "Loss: 0.006810936930567146\n",
      "Epoch: 7330\n",
      "Loss: 0.006810731418574692\n",
      "Epoch: 7340\n",
      "Loss: 0.006810526348734397\n",
      "Epoch: 7350\n",
      "Loss: 0.006810321719054284\n",
      "Epoch: 7360\n",
      "Loss: 0.0068101175275624625\n",
      "Epoch: 7370\n",
      "Loss: 0.006809913772306803\n",
      "Epoch: 7380\n",
      "Loss: 0.0068097104513546195\n",
      "Epoch: 7390\n",
      "Loss: 0.006809507562792365\n",
      "Epoch: 7400\n",
      "Loss: 0.0068093051047253194\n",
      "Epoch: 7410\n",
      "Loss: 0.006809103075277297\n",
      "Epoch: 7420\n",
      "Loss: 0.006808901472590345\n",
      "Epoch: 7430\n",
      "Loss: 0.006808700294824468\n",
      "Epoch: 7440\n",
      "Loss: 0.006808499540157331\n",
      "Epoch: 7450\n",
      "Loss: 0.006808299206784002\n",
      "Epoch: 7460\n",
      "Loss: 0.00680809929291666\n",
      "Epoch: 7470\n",
      "Loss: 0.006807899796784344\n",
      "Epoch: 7480\n",
      "Loss: 0.006807700716632688\n",
      "Epoch: 7490\n",
      "Loss: 0.006807502050723659\n",
      "Epoch: 7500\n",
      "Loss: 0.006807303797335314\n",
      "Epoch: 7510\n",
      "Loss: 0.0068071059547615475\n",
      "Epoch: 7520\n",
      "Loss: 0.006806908521311857\n",
      "Epoch: 7530\n",
      "Loss: 0.006806711495311094\n",
      "Epoch: 7540\n",
      "Loss: 0.0068065148750992436\n",
      "Epoch: 7550\n",
      "Loss: 0.006806318659031185\n",
      "Epoch: 7560\n",
      "Loss: 0.006806122845476481\n",
      "Epoch: 7570\n",
      "Loss: 0.006805927432819139\n",
      "Epoch: 7580\n",
      "Loss: 0.006805732419457413\n",
      "Epoch: 7590\n",
      "Loss: 0.006805537803803583\n",
      "Epoch: 7600\n",
      "Loss: 0.006805343584283748\n",
      "Epoch: 7610\n",
      "Loss: 0.006805149759337621\n",
      "Epoch: 7620\n",
      "Loss: 0.006804956327418331\n",
      "Epoch: 7630\n",
      "Loss: 0.006804763286992227\n",
      "Epoch: 7640\n",
      "Loss: 0.006804570636538681\n",
      "Epoch: 7650\n",
      "Loss: 0.0068043783745499005\n",
      "Epoch: 7660\n",
      "Loss: 0.006804186499530743\n",
      "Epoch: 7670\n",
      "Loss: 0.006803995009998539\n",
      "Epoch: 7680\n",
      "Loss: 0.006803803904482897\n",
      "Epoch: 7690\n",
      "Loss: 0.006803613181525543\n",
      "Epoch: 7700\n",
      "Loss: 0.006803422839680142\n",
      "Epoch: 7710\n",
      "Loss: 0.006803232877512127\n",
      "Epoch: 7720\n",
      "Loss: 0.006803043293598535\n",
      "Epoch: 7730\n",
      "Loss: 0.006802854086527844\n",
      "Epoch: 7740\n",
      "Loss: 0.0068026652548998055\n",
      "Epoch: 7750\n",
      "Loss: 0.006802476797325295\n",
      "Epoch: 7760\n",
      "Loss: 0.0068022887124261555\n",
      "Epoch: 7770\n",
      "Loss: 0.006802100998835043\n",
      "Epoch: 7780\n",
      "Loss: 0.006801913655195271\n",
      "Epoch: 7790\n",
      "Loss: 0.006801726680160682\n",
      "Epoch: 7800\n",
      "Loss: 0.0068015400723954855\n",
      "Epoch: 7810\n",
      "Loss: 0.006801353830574123\n",
      "Epoch: 7820\n",
      "Loss: 0.0068011679533811304\n",
      "Epoch: 7830\n",
      "Loss: 0.006800982439510998\n",
      "Epoch: 7840\n",
      "Loss: 0.0068007972876680395\n",
      "Epoch: 7850\n",
      "Loss: 0.00680061249656626\n",
      "Epoch: 7860\n",
      "Loss: 0.006800428064929221\n",
      "Epoch: 7870\n",
      "Loss: 0.0068002439914899215\n",
      "Epoch: 7880\n",
      "Loss: 0.006800060274990667\n",
      "Epoch: 7890\n",
      "Loss: 0.006799876914182947\n",
      "Epoch: 7900\n",
      "Loss: 0.00679969390782732\n",
      "Epoch: 7910\n",
      "Loss: 0.0067995112546932876\n",
      "Epoch: 7920\n",
      "Loss: 0.006799328953559181\n",
      "Epoch: 7930\n",
      "Loss: 0.006799147003212044\n",
      "Epoch: 7940\n",
      "Loss: 0.006798965402447528\n",
      "Epoch: 7950\n",
      "Loss: 0.006798784150069773\n",
      "Epoch: 7960\n",
      "Loss: 0.006798603244891303\n",
      "Epoch: 7970\n",
      "Loss: 0.0067984226857329185\n",
      "Epoch: 7980\n",
      "Loss: 0.006798242471423588\n",
      "Epoch: 7990\n",
      "Loss: 0.006798062600800351\n",
      "Epoch: 8000\n",
      "Loss: 0.006797883072708214\n",
      "Epoch: 8010\n",
      "Loss: 0.0067977038860000455\n",
      "Epoch: 8020\n",
      "Loss: 0.0067975250395364865\n",
      "Epoch: 8030\n",
      "Loss: 0.006797346532185849\n",
      "Epoch: 8040\n",
      "Loss: 0.006797168362824022\n",
      "Epoch: 8050\n",
      "Loss: 0.0067969905303343755\n",
      "Epoch: 8060\n",
      "Loss: 0.006796813033607677\n",
      "Epoch: 8070\n",
      "Loss: 0.006796635871541993\n",
      "Epoch: 8080\n",
      "Loss: 0.006796459043042602\n",
      "Epoch: 8090\n",
      "Loss: 0.006796282547021912\n",
      "Epoch: 8100\n",
      "Loss: 0.006796106382399369\n",
      "Epoch: 8110\n",
      "Loss: 0.006795930548101375\n",
      "Epoch: 8120\n",
      "Loss: 0.00679575504306121\n",
      "Epoch: 8130\n",
      "Loss: 0.006795579866218939\n",
      "Epoch: 8140\n",
      "Loss: 0.0067954050165213425\n",
      "Epoch: 8150\n",
      "Loss: 0.006795230492921834\n",
      "Epoch: 8160\n",
      "Loss: 0.00679505629438038\n",
      "Epoch: 8170\n",
      "Loss: 0.0067948824198634265\n",
      "Epoch: 8180\n",
      "Loss: 0.006794708868343824\n",
      "Epoch: 8190\n",
      "Loss: 0.006794535638800751\n",
      "Epoch: 8200\n",
      "Loss: 0.006794362730219644\n",
      "Epoch: 8210\n",
      "Loss: 0.006794190141592127\n",
      "Epoch: 8220\n",
      "Loss: 0.006794017871915934\n",
      "Epoch: 8230\n",
      "Loss: 0.00679384592019485\n",
      "Epoch: 8240\n",
      "Loss: 0.006793674285438635\n",
      "Epoch: 8250\n",
      "Loss: 0.006793502966662961\n",
      "Epoch: 8260\n",
      "Loss: 0.006793331962889343\n",
      "Epoch: 8270\n",
      "Loss: 0.006793161273145078\n",
      "Epoch: 8280\n",
      "Loss: 0.006792990896463177\n",
      "Epoch: 8290\n",
      "Loss: 0.006792820831882303\n",
      "Epoch: 8300\n",
      "Loss: 0.0067926510784467155\n",
      "Epoch: 8310\n",
      "Loss: 0.006792481635206197\n",
      "Epoch: 8320\n",
      "Loss: 0.006792312501216006\n",
      "Epoch: 8330\n",
      "Loss: 0.006792143675536807\n",
      "Epoch: 8340\n",
      "Loss: 0.006791975157234623\n",
      "Epoch: 8350\n",
      "Loss: 0.006791806945380767\n",
      "Epoch: 8360\n",
      "Loss: 0.006791639039051794\n",
      "Epoch: 8370\n",
      "Loss: 0.0067914714373294455\n",
      "Epoch: 8380\n",
      "Loss: 0.006791304139300586\n",
      "Epoch: 8390\n",
      "Loss: 0.006791137144057161\n",
      "Epoch: 8400\n",
      "Loss: 0.006790970450696137\n",
      "Epoch: 8410\n",
      "Loss: 0.006790804058319449\n",
      "Epoch: 8420\n",
      "Loss: 0.006790637966033954\n",
      "Epoch: 8430\n",
      "Loss: 0.006790472172951375\n",
      "Epoch: 8440\n",
      "Loss: 0.006790306678188252\n",
      "Epoch: 8450\n",
      "Loss: 0.0067901414808659005\n",
      "Epoch: 8460\n",
      "Loss: 0.006789976580110352\n",
      "Epoch: 8470\n",
      "Loss: 0.00678981197505231\n",
      "Epoch: 8480\n",
      "Loss: 0.006789647664827114\n",
      "Epoch: 8490\n",
      "Loss: 0.00678948364857467\n",
      "Epoch: 8500\n",
      "Loss: 0.006789319925439434\n",
      "Epoch: 8510\n",
      "Loss: 0.006789156494570338\n",
      "Epoch: 8520\n",
      "Loss: 0.0067889933551207714\n",
      "Epoch: 8530\n",
      "Loss: 0.006788830506248517\n",
      "Epoch: 8540\n",
      "Loss: 0.006788667947115727\n",
      "Epoch: 8550\n",
      "Loss: 0.0067885056768888586\n",
      "Epoch: 8560\n",
      "Loss: 0.006788343694738654\n",
      "Epoch: 8570\n",
      "Loss: 0.0067881819998400865\n",
      "Epoch: 8580\n",
      "Loss: 0.006788020591372326\n",
      "Epoch: 8590\n",
      "Loss: 0.0067878594685186885\n",
      "Epoch: 8600\n",
      "Loss: 0.006787698630466618\n",
      "Epoch: 8610\n",
      "Loss: 0.006787538076407625\n",
      "Epoch: 8620\n",
      "Loss: 0.006787377805537261\n",
      "Epoch: 8630\n",
      "Loss: 0.006787217817055075\n",
      "Epoch: 8640\n",
      "Loss: 0.006787058110164584\n",
      "Epoch: 8650\n",
      "Loss: 0.006786898684073232\n",
      "Epoch: 8660\n",
      "Loss: 0.006786739537992348\n",
      "Epoch: 8670\n",
      "Loss: 0.006786580671137121\n",
      "Epoch: 8680\n",
      "Loss: 0.0067864220827265585\n",
      "Epoch: 8690\n",
      "Loss: 0.0067862637719834544\n",
      "Epoch: 8700\n",
      "Loss: 0.006786105738134351\n",
      "Epoch: 8710\n",
      "Loss: 0.006785947980409513\n",
      "Epoch: 8720\n",
      "Loss: 0.006785790498042886\n",
      "Epoch: 8730\n",
      "Loss: 0.006785633290272071\n",
      "Epoch: 8740\n",
      "Loss: 0.006785476356338288\n",
      "Epoch: 8750\n",
      "Loss: 0.006785319695486342\n",
      "Epoch: 8760\n",
      "Loss: 0.006785163306964598\n",
      "Epoch: 8770\n",
      "Loss: 0.006785007190024944\n",
      "Epoch: 8780\n",
      "Loss: 0.006784851343922765\n",
      "Epoch: 8790\n",
      "Loss: 0.006784695767916915\n",
      "Epoch: 8800\n",
      "Loss: 0.006784540461269676\n",
      "Epoch: 8810\n",
      "Loss: 0.006784385423246739\n",
      "Epoch: 8820\n",
      "Loss: 0.006784230653117177\n",
      "Epoch: 8830\n",
      "Loss: 0.0067840761501534045\n",
      "Epoch: 8840\n",
      "Loss: 0.00678392191363116\n",
      "Epoch: 8850\n",
      "Loss: 0.006783767942829477\n",
      "Epoch: 8860\n",
      "Loss: 0.006783614237030651\n",
      "Epoch: 8870\n",
      "Loss: 0.006783460795520216\n",
      "Epoch: 8880\n",
      "Loss: 0.0067833076175869205\n",
      "Epoch: 8890\n",
      "Loss: 0.006783154702522698\n",
      "Epoch: 8900\n",
      "Loss: 0.006783002049622638\n",
      "Epoch: 8910\n",
      "Loss: 0.006782849658184968\n",
      "Epoch: 8920\n",
      "Loss: 0.006782697527511023\n",
      "Epoch: 8930\n",
      "Loss: 0.006782545656905217\n",
      "Epoch: 8940\n",
      "Loss: 0.006782394045675031\n",
      "Epoch: 8950\n",
      "Loss: 0.0067822426931309744\n",
      "Epoch: 8960\n",
      "Loss: 0.006782091598586567\n",
      "Epoch: 8970\n",
      "Loss: 0.0067819407613583195\n",
      "Epoch: 8980\n",
      "Loss: 0.006781790180765702\n",
      "Epoch: 8990\n",
      "Loss: 0.006781639856131126\n",
      "Epoch: 9000\n",
      "Loss: 0.00678148978677992\n",
      "Epoch: 9010\n",
      "Loss: 0.006781339972040308\n",
      "Epoch: 9020\n",
      "Loss: 0.006781190411243387\n",
      "Epoch: 9030\n",
      "Loss: 0.0067810411037231\n",
      "Epoch: 9040\n",
      "Loss: 0.006780892048816224\n",
      "Epoch: 9050\n",
      "Loss: 0.006780743245862341\n",
      "Epoch: 9060\n",
      "Loss: 0.006780594694203818\n",
      "Epoch: 9070\n",
      "Loss: 0.006780446393185787\n",
      "Epoch: 9080\n",
      "Loss: 0.006780298342156127\n",
      "Epoch: 9090\n",
      "Loss: 0.006780150540465439\n",
      "Epoch: 9100\n",
      "Loss: 0.006780002987467028\n",
      "Epoch: 9110\n",
      "Loss: 0.006779855682516881\n",
      "Epoch: 9120\n",
      "Loss: 0.0067797086249736516\n",
      "Epoch: 9130\n",
      "Loss: 0.006779561814198635\n",
      "Epoch: 9140\n",
      "Loss: 0.006779415249555756\n",
      "Epoch: 9150\n",
      "Loss: 0.006779268930411542\n",
      "Epoch: 9160\n",
      "Loss: 0.006779122856135109\n",
      "Epoch: 9170\n",
      "Loss: 0.006778977026098143\n",
      "Epoch: 9180\n",
      "Loss: 0.006778831439674879\n",
      "Epoch: 9190\n",
      "Loss: 0.006778686096242085\n",
      "Epoch: 9200\n",
      "Loss: 0.006778540995179044\n",
      "Epoch: 9210\n",
      "Loss: 0.006778396135867534\n",
      "Epoch: 9220\n",
      "Loss: 0.0067782515176918155\n",
      "Epoch: 9230\n",
      "Loss: 0.0067781071400386085\n",
      "Epoch: 9240\n",
      "Loss: 0.006777963002297077\n",
      "Epoch: 9250\n",
      "Loss: 0.0067778191038588165\n",
      "Epoch: 9260\n",
      "Loss: 0.0067776754441178306\n",
      "Epoch: 9270\n",
      "Loss: 0.006777532022470519\n",
      "Epoch: 9280\n",
      "Loss: 0.00677738883831566\n",
      "Epoch: 9290\n",
      "Loss: 0.00677724589105439\n",
      "Epoch: 9300\n",
      "Loss: 0.0067771031800902\n",
      "Epoch: 9310\n",
      "Loss: 0.006776960704828904\n",
      "Epoch: 9320\n",
      "Loss: 0.006776818464678633\n",
      "Epoch: 9330\n",
      "Loss: 0.006776676459049817\n",
      "Epoch: 9340\n",
      "Loss: 0.006776534687355171\n",
      "Epoch: 9350\n",
      "Loss: 0.006776393149009677\n",
      "Epoch: 9360\n",
      "Loss: 0.006776251843430576\n",
      "Epoch: 9370\n",
      "Loss: 0.0067761107700373404\n",
      "Epoch: 9380\n",
      "Loss: 0.006775969928251674\n",
      "Epoch: 9390\n",
      "Loss: 0.006775829317497484\n",
      "Epoch: 9400\n",
      "Loss: 0.0067756889372008805\n",
      "Epoch: 9410\n",
      "Loss: 0.006775548786790151\n",
      "Epoch: 9420\n",
      "Loss: 0.006775408865695751\n",
      "Epoch: 9430\n",
      "Loss: 0.0067752691733502885\n",
      "Epoch: 9440\n",
      "Loss: 0.006775129709188516\n",
      "Epoch: 9450\n",
      "Loss: 0.006774990472647306\n",
      "Epoch: 9460\n",
      "Loss: 0.0067748514631656475\n",
      "Epoch: 9470\n",
      "Loss: 0.006774712680184632\n",
      "Epoch: 9480\n",
      "Loss: 0.006774574123147431\n",
      "Epoch: 9490\n",
      "Loss: 0.006774435791499294\n",
      "Epoch: 9500\n",
      "Loss: 0.006774297684687531\n",
      "Epoch: 9510\n",
      "Loss: 0.006774159802161498\n",
      "Epoch: 9520\n",
      "Loss: 0.006774022143372587\n",
      "Epoch: 9530\n",
      "Loss: 0.006773884707774213\n",
      "Epoch: 9540\n",
      "Loss: 0.006773747494821803\n",
      "Epoch: 9550\n",
      "Loss: 0.006773610503972778\n",
      "Epoch: 9560\n",
      "Loss: 0.006773473734686549\n",
      "Epoch: 9570\n",
      "Loss: 0.006773337186424499\n",
      "Epoch: 9580\n",
      "Loss: 0.006773200858649973\n",
      "Epoch: 9590\n",
      "Loss: 0.006773064750828269\n",
      "Epoch: 9600\n",
      "Loss: 0.0067729288624266215\n",
      "Epoch: 9610\n",
      "Loss: 0.0067727931929141925\n",
      "Epoch: 9620\n",
      "Loss: 0.006772657741762059\n",
      "Epoch: 9630\n",
      "Loss: 0.006772522508443206\n",
      "Epoch: 9640\n",
      "Loss: 0.006772387492432508\n",
      "Epoch: 9650\n",
      "Loss: 0.006772252693206725\n",
      "Epoch: 9660\n",
      "Loss: 0.006772118110244485\n",
      "Epoch: 9670\n",
      "Loss: 0.006771983743026279\n",
      "Epoch: 9680\n",
      "Loss: 0.006771849591034448\n",
      "Epoch: 9690\n",
      "Loss: 0.006771715653753169\n",
      "Epoch: 9700\n",
      "Loss: 0.006771581930668449\n",
      "Epoch: 9710\n",
      "Loss: 0.006771448421268115\n",
      "Epoch: 9720\n",
      "Loss: 0.0067713151250418015\n",
      "Epoch: 9730\n",
      "Loss: 0.006771182041480936\n",
      "Epoch: 9740\n",
      "Loss: 0.00677104917007874\n",
      "Epoch: 9750\n",
      "Loss: 0.006770916510330203\n",
      "Epoch: 9760\n",
      "Loss: 0.006770784061732092\n",
      "Epoch: 9770\n",
      "Loss: 0.006770651823782925\n",
      "Epoch: 9780\n",
      "Loss: 0.006770519795982968\n",
      "Epoch: 9790\n",
      "Loss: 0.006770387977834227\n",
      "Epoch: 9800\n",
      "Loss: 0.006770256368840434\n",
      "Epoch: 9810\n",
      "Loss: 0.0067701249685070426\n",
      "Epoch: 9820\n",
      "Loss: 0.006769993776341211\n",
      "Epoch: 9830\n",
      "Loss: 0.006769862791851799\n",
      "Epoch: 9840\n",
      "Loss: 0.006769732014549362\n",
      "Epoch: 9850\n",
      "Loss: 0.006769601443946131\n",
      "Epoch: 9860\n",
      "Loss: 0.006769471079556009\n",
      "Epoch: 9870\n",
      "Loss: 0.006769340920894566\n",
      "Epoch: 9880\n",
      "Loss: 0.006769210967479024\n",
      "Epoch: 9890\n",
      "Loss: 0.006769081218828251\n",
      "Epoch: 9900\n",
      "Loss: 0.0067689516744627535\n",
      "Epoch: 9910\n",
      "Loss: 0.00676882233390466\n",
      "Epoch: 9920\n",
      "Loss: 0.0067686931966777246\n",
      "Epoch: 9930\n",
      "Loss: 0.00676856426230731\n",
      "Epoch: 9940\n",
      "Loss: 0.00676843553032038\n",
      "Epoch: 9950\n",
      "Loss: 0.006768307000245496\n",
      "Epoch: 9960\n",
      "Loss: 0.006768178671612798\n",
      "Epoch: 9970\n",
      "Loss: 0.006768050543954009\n",
      "Epoch: 9980\n",
      "Loss: 0.00676792261680242\n",
      "Epoch: 9990\n",
      "Loss: 0.006767794889692881\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack([np.random.normal(1, 0.5, [100,32]),\n",
    "                np.random.normal(-2, 1, [100,32]),\n",
    "                np.random.normal(3, 0.75, [100,32])])\n",
    "\n",
    "data = Data(X) # Cria um objeto Data com os dados X\n",
    "\n",
    "data.normalize() # Normaliza os dados\n",
    "data.shuffleData() # Embaralha as linhas\n",
    "data.setTrainRatio(0.8) # Define a proporção de dados de treino\n",
    "\n",
    "inputDim = data.getAttrSize() # Dimensão de entrada (número de atributos)\n",
    "\n",
    "# Número de neurônios em cada camada escondida\n",
    "# A última camada dessa lista é a dimensão latente\n",
    "# O espelhamento é feito automaticamente\n",
    "# Exemplo:\n",
    "# Para um X com 32 atributos, se hiddenLayers = [16, 8], a rede terá a seguinte estrutura:\n",
    "# [32, 16, 8, 16, 32]\n",
    "# Em que o primeiro valor é a dimensão de entrada e o último é a dimensão de saída\n",
    "hiddenLayers = [16, 8]\n",
    "\n",
    "# Cria o modelo do autoencoder\n",
    "autoencoder = createModel(inputDim, hiddenLayers)\n",
    "\n",
    "# Treina o autoencoder\n",
    "autoencoder.fit(data.getTrainData(), data.getTrainData(), lr=0.01, epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro quadrático médio: 0.704%\n"
     ]
    }
   ],
   "source": [
    "X = data.getTestData() # Dados de teste\n",
    "X_hat = autoencoder.predict(X) # Dados preditos pelo modelo com base nos dados de teste\n",
    "\n",
    "# Calcula o erro quadrático médio\n",
    "err = np.mean(np.square(X - X_hat))\n",
    "\n",
    "print(f'Erro quadrático médio: {err * 100:.3f}%')\n",
    "\n",
    "# autoencoder.getLatentSpace()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
