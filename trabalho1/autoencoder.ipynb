{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## João Pedro Rodrigues Freitas - 11316552"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Fazer os mini batches (train, test)\n",
    "- Fazer o one-hot\n",
    "- conferir normalização - OK\n",
    "- Não passar func ativ na ultima camada - OK\n",
    "- Fazer o espelhamento automático - OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, X: np.ndarray) -> None:\n",
    "        self.data = X\n",
    "\n",
    "    def getData(self) -> np.ndarray:\n",
    "        return self.data\n",
    "\n",
    "    def setTrainRatio(self, ratio: float) -> None:\n",
    "        self.train_ratio = ratio\n",
    "\n",
    "    def getTrainRatio(self) -> float:\n",
    "        return self.train_ratio\n",
    "    \n",
    "    def getAttrSize(self) -> int:\n",
    "        return self.data.shape[1]\n",
    "    \n",
    "    def shuffleData(self) -> None:\n",
    "        self.data = np.random.permutation(self.data)\n",
    "\n",
    "    def getTrainData(self) -> np.ndarray:\n",
    "        return self.data[:int(self.data.shape[0] * self.train_ratio)]\n",
    "    \n",
    "    def getTestData(self) -> np.ndarray:\n",
    "        return self.data[int(self.data.shape[0] * self.train_ratio):]\n",
    "\n",
    "    def normalize(self) -> None:\n",
    "        '''Normaliza cada atributo para o intervalo [0, 1]'''\n",
    "        for i in range(self.getAttrSize()):\n",
    "            self.data[:,i] = (self.data[:,i] - np.min(self.data[:,i])) / (np.max(self.data[:,i]) - np.min(self.data[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, n_neurons: int, actv_func, inputs = None, lastLayer = False) -> None:\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.shape = None\n",
    "        self.inputShape = None\n",
    "\n",
    "        self.actv_func = actv_func\n",
    "\n",
    "        self.out = None\n",
    "        self.actv = None\n",
    "\n",
    "        self.lastLayer = lastLayer\n",
    "\n",
    "        self._setWeights()\n",
    "\n",
    "    def _setWeights(self) -> None:  \n",
    "        if self.W is None and self.inputs is not None:\n",
    "            self.inputShape = self.inputs.shape\n",
    "\n",
    "            self.shape = (self.inputShape[-1], self.n_neurons)\n",
    "            # Inicializa os pesos e bias com distribuição uniforme\n",
    "            # entre -0.5 e 0.5\n",
    "            self.W = np.random.rand(self.shape[0], self.shape[1]) - 0.5\n",
    "            self.b = np.random.rand(self.n_neurons) - 0.5\n",
    "\n",
    "    def process(self, inputs):\n",
    "        '''Forward da camada'''\n",
    "        self.inputs = inputs\n",
    "        self._setWeights()\n",
    "        self.out = np.dot(self.inputs, self.W) + self.b\n",
    "        if not self.lastLayer:\n",
    "            self.actv = self.actv_func(self.out)\n",
    "        else:\n",
    "            self.actv = self.out\n",
    "\n",
    "    def getOutput(self):\n",
    "        return self.out\n",
    "    \n",
    "    def getActivation(self):\n",
    "        return self.actv\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    def __init__(self) -> None:\n",
    "        self.layers = np.array([], dtype=Layer)\n",
    "\n",
    "        self.epochs = None\n",
    "        self.lr = None\n",
    "\n",
    "        self.inputs = None # X\n",
    "        self.targets = None # X\n",
    "\n",
    "        self.outputs = []\n",
    "        self.ativations = [] # X_Hat\n",
    "\n",
    "    def addLayer(self, layer: Layer) -> None:\n",
    "        self.layers = np.append(self.layers, layer)\n",
    "\n",
    "    # TODO: separar entre encoder e decoder\n",
    "    def forward(self, inputs):\n",
    "        self.ativations = []\n",
    "        self.outputs = []\n",
    "\n",
    "        self.ativations.append(inputs)\n",
    "        self.outputs.append(inputs)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.process(inputs)\n",
    "            inputs = layer.getActivation()\n",
    "            output = layer.getOutput()\n",
    "\n",
    "            self.outputs.append(output)\n",
    "            self.ativations.append(inputs)\n",
    "\n",
    "            # inputs = layer.getActivation()\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    # TODO: separar entre encoder e decoder\n",
    "    def backward(self) -> None:\n",
    "        i = self.layers.size\n",
    "        n = self.inputs.shape[0]\n",
    "\n",
    "        err = 2 * (self.targets - self.ativations[-1]) # x - x_hat\n",
    "\n",
    "        # TODO: dividir por n_samples\n",
    "\n",
    "        for layer, z in zip(self.layers[::-1], self.outputs[::-1]):\n",
    "            delta = err\n",
    "\n",
    "            if (i != self.layers.size):\n",
    "                delta *= layer.actv_func(z, derivative=True)\n",
    "\n",
    "            # delta = err * layer.actv_func(x_hat, derivative=True)\n",
    "\n",
    "            err = np.dot(delta, layer.W.T) # Atualiza o erro pro layer anterior\n",
    "\n",
    "            if i > 0:\n",
    "                prev_ativ = self.ativations[i-1]\n",
    "                # dW = np.dot(prev_output.T, delta) / n\n",
    "                # db = np.sum(delta, axis=0) / n\n",
    "                dW = np.dot(prev_ativ.T, delta) / n\n",
    "                db = delta.mean()\n",
    "\n",
    "                layer.W += self.lr * dW\n",
    "                layer.b += self.lr * db\n",
    "\n",
    "\n",
    "            i -= 1\n",
    "        \n",
    "    def fit(self, inputs, targets, lr: float = 0.01, epochs: int = 100) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.lr = lr\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(inputs)\n",
    "            self.backward()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch: {epoch}')\n",
    "                print(f'Loss: {np.mean(np.square(self.targets - self.ativations[-1]))}')\n",
    "\n",
    "\n",
    "    def predict(self, inputs) -> np.ndarray:\n",
    "        self.forward(inputs)\n",
    "        return self.ativations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "    positives = x >= 0\n",
    "    negatives = ~positives\n",
    "    \n",
    "    exp_x_neg = np.exp(x[negatives])\n",
    "    \n",
    "    y = x.copy()\n",
    "    y[positives] = 1 / (1 + np.exp(-x[positives]))\n",
    "    y[negatives] = exp_x_neg / (1 + exp_x_neg)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def createModel(inputDim, layers, actv_func):\n",
    "    model = Autoencoder()\n",
    "\n",
    "    # Adiciona as camadas do encoder\n",
    "    for i in range(len(layers)):\n",
    "        model.addLayer(Layer(layers[i], actv_func))\n",
    "\n",
    "    # Adiciona as camadas do decoder\n",
    "    for i in range(len(layers)-2, -1, -1):\n",
    "        model.addLayer(Layer(layers[i], actv_func))\n",
    "\n",
    "    # Adiciona a camada de saída\n",
    "    model.addLayer(Layer(inputDim, actv_func, lastLayer = True))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 1.174781053735059\n",
      "Epoch: 10\n",
      "Loss: 0.07346832254839122\n",
      "Epoch: 20\n",
      "Loss: 0.054883106829943795\n",
      "Epoch: 30\n",
      "Loss: 0.054287715813658356\n",
      "Epoch: 40\n",
      "Loss: 0.054109685520874605\n",
      "Epoch: 50\n",
      "Loss: 0.05393917623647268\n",
      "Epoch: 60\n",
      "Loss: 0.05376554353960545\n",
      "Epoch: 70\n",
      "Loss: 0.053588085940637985\n",
      "Epoch: 80\n",
      "Loss: 0.05340638222233869\n",
      "Epoch: 90\n",
      "Loss: 0.05322004302429386\n",
      "Epoch: 100\n",
      "Loss: 0.05302870316946081\n",
      "Epoch: 110\n",
      "Loss: 0.05283201995933295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120\n",
      "Loss: 0.05262967127279914\n",
      "Epoch: 130\n",
      "Loss: 0.05242135322555697\n",
      "Epoch: 140\n",
      "Loss: 0.05220677731291112\n",
      "Epoch: 150\n",
      "Loss: 0.05198566698420885\n",
      "Epoch: 160\n",
      "Loss: 0.05175775362855188\n",
      "Epoch: 170\n",
      "Loss: 0.05152277199142546\n",
      "Epoch: 180\n",
      "Loss: 0.05128045508772273\n",
      "Epoch: 190\n",
      "Loss: 0.051030528724017234\n",
      "Epoch: 200\n",
      "Loss: 0.05077270578673397\n",
      "Epoch: 210\n",
      "Loss: 0.050506680487994976\n",
      "Epoch: 220\n",
      "Loss: 0.050232122783203416\n",
      "Epoch: 230\n",
      "Loss: 0.049948673181400734\n",
      "Epoch: 240\n",
      "Loss: 0.0496559381607544\n",
      "Epoch: 250\n",
      "Loss: 0.04935348637905796\n",
      "Epoch: 260\n",
      "Loss: 0.04904084583650721\n",
      "Epoch: 270\n",
      "Loss: 0.048717502110027455\n",
      "Epoch: 280\n",
      "Loss: 0.04838289774012739\n",
      "Epoch: 290\n",
      "Loss: 0.04803643281719464\n",
      "Epoch: 300\n",
      "Loss: 0.04767746678776938\n",
      "Epoch: 310\n",
      "Loss: 0.04730532148462741\n",
      "Epoch: 320\n",
      "Loss: 0.046919285377941086\n",
      "Epoch: 330\n",
      "Loss: 0.04651861904745416\n",
      "Epoch: 340\n",
      "Loss: 0.04610256188545928\n",
      "Epoch: 350\n",
      "Loss: 0.04567034005450941\n",
      "Epoch: 360\n",
      "Loss: 0.04522117573871775\n",
      "Epoch: 370\n",
      "Loss: 0.04475429773923663\n",
      "Epoch: 380\n",
      "Loss: 0.04426895346873484\n",
      "Epoch: 390\n",
      "Loss: 0.0437644223918459\n",
      "Epoch: 400\n",
      "Loss: 0.04324003093400245\n",
      "Epoch: 410\n",
      "Loss: 0.04269516883550296\n",
      "Epoch: 420\n",
      "Loss: 0.04212930685776725\n",
      "Epoch: 430\n",
      "Loss: 0.04154201565320402\n",
      "Epoch: 440\n",
      "Loss: 0.040932985490805804\n",
      "Epoch: 450\n",
      "Loss: 0.040302046392782724\n",
      "Epoch: 460\n",
      "Loss: 0.039649188094723266\n",
      "Epoch: 470\n",
      "Loss: 0.038974579109481766\n",
      "Epoch: 480\n",
      "Loss: 0.0382785840733455\n",
      "Epoch: 490\n",
      "Loss: 0.03756177850257973\n",
      "Epoch: 500\n",
      "Loss: 0.03682496010580881\n",
      "Epoch: 510\n",
      "Loss: 0.036069155890889736\n",
      "Epoch: 520\n",
      "Loss: 0.035295624469901324\n",
      "Epoch: 530\n",
      "Loss: 0.03450585318561524\n",
      "Epoch: 540\n",
      "Loss: 0.033701549930145146\n",
      "Epoch: 550\n",
      "Loss: 0.032884629769913395\n",
      "Epoch: 560\n",
      "Loss: 0.03205719670234228\n",
      "Epoch: 570\n",
      "Loss: 0.031221521030233138\n",
      "Epoch: 580\n",
      "Loss: 0.03038001294430974\n",
      "Epoch: 590\n",
      "Loss: 0.029535192960064998\n",
      "Epoch: 600\n",
      "Loss: 0.02868965987735476\n",
      "Epoch: 610\n",
      "Loss: 0.027846056937975847\n",
      "Epoch: 620\n",
      "Loss: 0.027007036862467325\n",
      "Epoch: 630\n",
      "Loss: 0.026175226460717623\n",
      "Epoch: 640\n",
      "Loss: 0.02535319153230146\n",
      "Epoch: 650\n",
      "Loss: 0.02454340279621321\n",
      "Epoch: 660\n",
      "Loss: 0.02374820360636542\n",
      "Epoch: 670\n",
      "Loss: 0.022969780208257254\n",
      "Epoch: 680\n",
      "Loss: 0.02221013526407382\n",
      "Epoch: 690\n",
      "Loss: 0.021471065311421695\n",
      "Epoch: 700\n",
      "Loss: 0.02075414272201879\n",
      "Epoch: 710\n",
      "Loss: 0.020060702592281748\n",
      "Epoch: 720\n",
      "Loss: 0.01939183483343955\n",
      "Epoch: 730\n",
      "Loss: 0.018748381543754654\n",
      "Epoch: 740\n",
      "Loss: 0.018130939551539934\n",
      "Epoch: 750\n",
      "Loss: 0.017539867828162695\n",
      "Epoch: 760\n",
      "Loss: 0.01697529929821972\n",
      "Epoch: 770\n",
      "Loss: 0.016437156431025907\n",
      "Epoch: 780\n",
      "Loss: 0.01592516989212288\n",
      "Epoch: 790\n",
      "Loss: 0.015438899470694588\n",
      "Epoch: 800\n",
      "Loss: 0.014977756479680748\n",
      "Epoch: 810\n",
      "Loss: 0.014541026847441397\n",
      "Epoch: 820\n",
      "Loss: 0.014127894177463574\n",
      "Epoch: 830\n",
      "Loss: 0.013737462138131448\n",
      "Epoch: 840\n",
      "Loss: 0.013368775649245898\n",
      "Epoch: 850\n",
      "Loss: 0.013020840446914836\n",
      "Epoch: 860\n",
      "Loss: 0.01269264072547662\n",
      "Epoch: 870\n",
      "Loss: 0.012383154667373811\n",
      "Epoch: 880\n",
      "Loss: 0.012091367774066953\n",
      "Epoch: 890\n",
      "Loss: 0.011816283999563653\n",
      "Epoch: 900\n",
      "Loss: 0.011556934760935961\n",
      "Epoch: 910\n",
      "Loss: 0.011312385956698414\n",
      "Epoch: 920\n",
      "Loss: 0.011081743164615819\n",
      "Epoch: 930\n",
      "Loss: 0.010864155216702738\n",
      "Epoch: 940\n",
      "Loss: 0.010658816362678156\n",
      "Epoch: 950\n",
      "Loss: 0.010464967236026416\n",
      "Epoch: 960\n",
      "Loss: 0.010281894831233068\n",
      "Epoch: 970\n",
      "Loss: 0.01010893168877621\n",
      "Epoch: 980\n",
      "Loss: 0.009945454467948766\n",
      "Epoch: 990\n",
      "Loss: 0.009790882068221037\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack([np.random.normal(1, 0.5, [100,64]),\n",
    "                np.random.normal(-2, 1, [100,64]),\n",
    "                np.random.normal(3, 0.75, [100,64])])\n",
    "\n",
    "data = Data(X)\n",
    "\n",
    "data.normalize()\n",
    "data.shuffleData()\n",
    "data.setTrainRatio(0.8)\n",
    "\n",
    "inputDim = data.getAttrSize()\n",
    "\n",
    "# Número de neurônios em cada camada escondida\n",
    "# A última camada dessa lista é a dimensão latente\n",
    "# O espelhamento é feito automaticamente\n",
    "# Exemplo:\n",
    "# Para um X com 64 atributos, se hiddenLayers = [32, 16], a rede terá a seguinte estrutura:\n",
    "# [64, 32, 16, 32, 64]\n",
    "# Em que o primeiro valor é a dimensão de entrada e o último é a dimensão de saída\n",
    "hiddenLayers = [32, 16]\n",
    "\n",
    "autoencoder = createModel(inputDim, hiddenLayers, actv_func=sigmoid)\n",
    "\n",
    "# autoencoder = createModel([inputDim] + teste + [inputDim], actv_func=sigmoid)\n",
    "autoencoder.fit(data.getTrainData(), data.getTrainData(), lr=0.01, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro quadrático médio: 0.883%\n"
     ]
    }
   ],
   "source": [
    "X = data.getTestData()\n",
    "X_hat = autoencoder.predict(X)\n",
    "\n",
    "err = np.mean(np.square(X - X_hat))\n",
    "\n",
    "print(f'Erro quadrático médio: {err * 100:.3f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
